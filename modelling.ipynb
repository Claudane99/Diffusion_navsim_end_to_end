{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba2ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append('/home/claudane/Desktop/navsim/')\n",
    "\n",
    "import hydra\n",
    "from hydra.utils import instantiate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ[\"NAVSIM_EXP_ROOT\"] =\"/home/claudane/Desktop/navsim/exp\"\n",
    "os.environ[\"NAVSIM_DEVKIT_ROOT\"] =\"/home/claudane/Desktop/navsim/\"\n",
    "os.environ[\"OPENSCENE_DATA_ROOT\"] =\"/home/claudane/Desktop/navsim/dataset\"\n",
    "os.environ[\"NUPLAN_MAPS_ROOT\"] = \"/home/claudane/Desktop/navsim/dataset/maps\"\n",
    "os.environ[\"CACHEDATA\"] = str(\"/home/claudane/Desktop/navsim/exp/training_cache\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c776b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataclasses\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import io\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from dataclasses import asdict, dataclass, fields\n",
    "from pathlib import Path\n",
    "from typing import Any, BinaryIO, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from nuplan.common.actor_state.state_representation import StateSE2\n",
    "from nuplan.common.maps.abstract_map import AbstractMap\n",
    "from nuplan.common.maps.maps_datatypes import TrafficLightStatuses\n",
    "from nuplan.common.maps.nuplan_map.map_factory import get_maps_api\n",
    "from nuplan.database.maps_db.gpkg_mapsdb import MAP_LOCATIONS\n",
    "from nuplan.database.utils.pointclouds.lidar import LidarPointCloud\n",
    "from nuplan.planning.simulation.observation.observation_type import DetectionsTracks\n",
    "from nuplan.planning.simulation.trajectory.trajectory_sampling import TrajectorySampling\n",
    "from PIL import Image\n",
    "from pyquaternion import Quaternion\n",
    "\n",
    "from navsim.planning.simulation.planner.pdm_planner.utils.pdm_geometry_utils import (\n",
    "    convert_absolute_to_relative_se2_array,\n",
    ")\n",
    "\n",
    "NAVSIM_INTERVAL_LENGTH: float = 0.5\n",
    "OPENSCENE_DATA_ROOT = os.environ.get(\"OPENSCENE_DATA_ROOT\")\n",
    "NUPLAN_MAPS_ROOT = os.environ.get(\"NUPLAN_MAPS_ROOT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc062e74",
   "metadata": {},
   "source": [
    "# Config / Dataloader / Dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1195033f",
   "metadata": {},
   "source": [
    "## Dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06138a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataclasses\n",
    "\n",
    "@dataclass\n",
    "class Camera:\n",
    "    \"\"\"Camera dataclass for image and parameters.\"\"\"\n",
    "\n",
    "    image: Optional[npt.NDArray[np.float32]] = None\n",
    "\n",
    "    sensor2lidar_rotation: Optional[npt.NDArray[np.float32]] = None\n",
    "    sensor2lidar_translation: Optional[npt.NDArray[np.float32]] = None\n",
    "    intrinsics: Optional[npt.NDArray[np.float32]] = None\n",
    "    distortion: Optional[npt.NDArray[np.float32]] = None\n",
    "\n",
    "    camera_path: Optional[Path] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Cameras:\n",
    "    \"\"\"Multi-camera dataclass.\"\"\"\n",
    "\n",
    "    cam_f0: Camera\n",
    "    cam_l0: Camera\n",
    "    cam_l1: Camera\n",
    "    cam_l2: Camera\n",
    "    cam_r0: Camera\n",
    "    cam_r1: Camera\n",
    "    cam_r2: Camera\n",
    "    cam_b0: Camera\n",
    "\n",
    "    @classmethod\n",
    "    def from_camera_dict(\n",
    "        cls,\n",
    "        sensor_blobs_path: Path,\n",
    "        camera_dict: Dict[str, Any],\n",
    "        sensor_names: List[str],\n",
    "    ) -> Cameras:\n",
    "        \"\"\"\n",
    "        Load camera dataclass from dictionary.\n",
    "        :param sensor_blobs_path: root directory of sensor data.\n",
    "        :param camera_dict: dictionary containing camera specifications.\n",
    "        :param sensor_names: list of camera identifiers to include.\n",
    "        :return: Cameras dataclass.\n",
    "        \"\"\"\n",
    "\n",
    "        data_dict: Dict[str, Camera] = {}\n",
    "        for camera_name in camera_dict.keys():\n",
    "            camera_identifier = camera_name.lower()\n",
    "            if camera_identifier in sensor_names:\n",
    "                image_path = sensor_blobs_path / camera_dict[camera_name][\"data_path\"]\n",
    "                data_dict[camera_identifier] = Camera(\n",
    "                    image=np.array(Image.open(image_path)),\n",
    "                    sensor2lidar_rotation=camera_dict[camera_name][\"sensor2lidar_rotation\"],\n",
    "                    sensor2lidar_translation=camera_dict[camera_name][\"sensor2lidar_translation\"],\n",
    "                    intrinsics=camera_dict[camera_name][\"cam_intrinsic\"],\n",
    "                    distortion=camera_dict[camera_name][\"distortion\"],\n",
    "                    camera_path=camera_dict[camera_name][\"data_path\"],\n",
    "                )\n",
    "            else:\n",
    "                data_dict[camera_identifier] = Camera()  # empty camera\n",
    "\n",
    "        return Cameras(\n",
    "            cam_f0=data_dict[\"cam_f0\"],\n",
    "            cam_l0=data_dict[\"cam_l0\"],\n",
    "            cam_l1=data_dict[\"cam_l1\"],\n",
    "            cam_l2=data_dict[\"cam_l2\"],\n",
    "            cam_r0=data_dict[\"cam_r0\"],\n",
    "            cam_r1=data_dict[\"cam_r1\"],\n",
    "            cam_r2=data_dict[\"cam_r2\"],\n",
    "            cam_b0=data_dict[\"cam_b0\"],\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Lidar:\n",
    "    \"\"\"Lidar point cloud dataclass.\"\"\"\n",
    "\n",
    "    # NOTE:\n",
    "    # merged lidar point cloud as (6,n) float32 array with n points\n",
    "    # first axis: (x, y, z, intensity, ring, lidar_id), see LidarIndex\n",
    "    lidar_pc: Optional[npt.NDArray[np.float32]] = None\n",
    "    lidar_path: Optional[Path] = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_bytes(lidar_path: Path) -> BinaryIO:\n",
    "        \"\"\"Helper static method to load lidar point cloud stream.\"\"\"\n",
    "        with open(lidar_path, \"rb\") as fp:\n",
    "            return io.BytesIO(fp.read())\n",
    "\n",
    "    @classmethod\n",
    "    def from_paths(cls, sensor_blobs_path: Path, lidar_path: Path, sensor_names: List[str]) -> Lidar:\n",
    "        \"\"\"\n",
    "        Loads lidar point cloud dataclass in log loading.\n",
    "        :param sensor_blobs_path: root directory to sensor data\n",
    "        :param lidar_path: relative lidar path from logs.\n",
    "        :param sensor_names: list of sensor identifiers to load`\n",
    "        :return: lidar point cloud dataclass\n",
    "        \"\"\"\n",
    "\n",
    "        # NOTE: this could be extended to load specific LiDARs in the merged pc\n",
    "        if \"lidar_pc\" in sensor_names:\n",
    "            global_lidar_path = sensor_blobs_path / lidar_path\n",
    "            lidar_pc = LidarPointCloud.from_buffer(cls._load_bytes(global_lidar_path), \"pcd\").points\n",
    "            return Lidar(lidar_pc, lidar_path)\n",
    "        return Lidar()  # empty lidar\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EgoStatus:\n",
    "    \"\"\"Ego vehicle status dataclass.\"\"\"\n",
    "\n",
    "    ego_pose: npt.NDArray[np.float64]\n",
    "    ego_velocity: npt.NDArray[np.float32]\n",
    "    ego_acceleration: npt.NDArray[np.float32]\n",
    "    driving_command: npt.NDArray[np.int]\n",
    "    in_global_frame: bool = False  # False for AgentInput\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AgentInput:\n",
    "    \"\"\"Dataclass for agent inputs with current and past ego statuses and sensors.\"\"\"\n",
    "\n",
    "    ego_statuses: List[EgoStatus]\n",
    "    cameras: List[Cameras]\n",
    "    lidars: List[Lidar]\n",
    "\n",
    "    @classmethod\n",
    "    def from_scene_dict_list(\n",
    "        cls,\n",
    "        scene_dict_list: List[Dict],\n",
    "        sensor_blobs_path: Path,\n",
    "        num_history_frames: int,\n",
    "        sensor_config: SensorConfig,\n",
    "    ) -> AgentInput:\n",
    "        \"\"\"\n",
    "        Load agent input from scene dictionary.\n",
    "        :param scene_dict_list: list of scene frames (in logs).\n",
    "        :param sensor_blobs_path: root directory of sensor data\n",
    "        :param num_history_frames: number of agent input frames\n",
    "        :param sensor_config: sensor config dataclass\n",
    "        :return: agent input dataclass\n",
    "        \"\"\"\n",
    "        assert len(scene_dict_list) > 0, \"Scene list is empty!\"\n",
    "\n",
    "        global_ego_poses = []\n",
    "        for frame_idx in range(num_history_frames):\n",
    "            ego_translation = scene_dict_list[frame_idx][\"ego2global_translation\"]\n",
    "            ego_quaternion = Quaternion(*scene_dict_list[frame_idx][\"ego2global_rotation\"])\n",
    "            global_ego_pose = np.array(\n",
    "                [\n",
    "                    ego_translation[0],\n",
    "                    ego_translation[1],\n",
    "                    ego_quaternion.yaw_pitch_roll[0],\n",
    "                ],\n",
    "                dtype=np.float64,\n",
    "            )\n",
    "            global_ego_poses.append(global_ego_pose)\n",
    "\n",
    "        local_ego_poses = convert_absolute_to_relative_se2_array(\n",
    "            StateSE2(*global_ego_poses[-1]),\n",
    "            np.array(global_ego_poses, dtype=np.float64),\n",
    "        )\n",
    "\n",
    "        ego_statuses: List[EgoStatus] = []\n",
    "        cameras: List[EgoStatus] = []\n",
    "        lidars: List[Lidar] = []\n",
    "\n",
    "        for frame_idx in range(num_history_frames):\n",
    "\n",
    "            ego_dynamic_state = scene_dict_list[frame_idx][\"ego_dynamic_state\"]\n",
    "            ego_status = EgoStatus(\n",
    "                ego_pose=np.array(local_ego_poses[frame_idx], dtype=np.float32),\n",
    "                ego_velocity=np.array(ego_dynamic_state[:2], dtype=np.float32),\n",
    "                ego_acceleration=np.array(ego_dynamic_state[2:], dtype=np.float32),\n",
    "                driving_command=scene_dict_list[frame_idx][\"driving_command\"],\n",
    "            )\n",
    "            ego_statuses.append(ego_status)\n",
    "\n",
    "            sensor_names = sensor_config.get_sensors_at_iteration(frame_idx)\n",
    "            cameras.append(\n",
    "                Cameras.from_camera_dict(\n",
    "                    sensor_blobs_path=sensor_blobs_path,\n",
    "                    camera_dict=scene_dict_list[frame_idx][\"cams\"],\n",
    "                    sensor_names=sensor_names,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            lidars.append(\n",
    "                Lidar.from_paths(\n",
    "                    sensor_blobs_path=sensor_blobs_path,\n",
    "                    lidar_path=Path(scene_dict_list[frame_idx][\"lidar_path\"]) if scene_dict_list[frame_idx][\"lidar_path\"] is not None else None,\n",
    "                    sensor_names=sensor_names,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return AgentInput(ego_statuses, cameras, lidars)\n",
    "\n",
    "    @classmethod\n",
    "    def from_scene_dict_list_private(\n",
    "        cls,\n",
    "        scene_dict_list: List[Dict],\n",
    "        sensor_blobs_path: Path,\n",
    "        num_history_frames: int,\n",
    "        sensor_config: SensorConfig,\n",
    "    ) -> AgentInput:\n",
    "        \"\"\"\n",
    "        Load agent input from scene dictionary.\n",
    "        :param scene_dict_list: list of scene frames (in logs).\n",
    "        :param sensor_blobs_path: root directory of sensor data\n",
    "        :param num_history_frames: number of agent input frames\n",
    "        :param sensor_config: sensor config dataclass\n",
    "        :return: agent input dataclass\n",
    "        \"\"\"\n",
    "        assert len(scene_dict_list) > 0, \"Scene list is empty!\"\n",
    "\n",
    "        ego_statuses: List[EgoStatus] = []\n",
    "        cameras: List[EgoStatus] = []\n",
    "        lidars: List[Lidar] = []\n",
    "\n",
    "        for frame_idx in range(num_history_frames):\n",
    "            ego_statuses.append(scene_dict_list[frame_idx].ego_status)\n",
    "            cameras.append(\n",
    "                    scene_dict_list[frame_idx].cameras\n",
    "            )\n",
    "            lidars.append(\n",
    "                    scene_dict_list[frame_idx].lidar\n",
    "            )\n",
    "\n",
    "        return AgentInput(ego_statuses, cameras, lidars)\n",
    "\n",
    "@dataclass\n",
    "class Annotations:\n",
    "    \"\"\"Dataclass of annotations (e.g. bounding boxes) per frame.\"\"\"\n",
    "\n",
    "    boxes: npt.NDArray[np.float32]\n",
    "    names: List[str]\n",
    "    velocity_3d: npt.NDArray[np.float32]\n",
    "    instance_tokens: List[str]\n",
    "    track_tokens: List[str]\n",
    "\n",
    "    def __post_init__(self):\n",
    "        annotation_lengths: Dict[str, int] = {\n",
    "            attribute_name: len(attribute) for attribute_name, attribute in vars(self).items()\n",
    "        }\n",
    "        assert (\n",
    "            len(set(annotation_lengths.values())) == 1\n",
    "        ), f\"Annotations expects all attributes to have equal length, but got {annotation_lengths}\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Trajectory:\n",
    "    \"\"\"Trajectory dataclass in NAVSIM.\"\"\"\n",
    "\n",
    "    poses: npt.NDArray[np.float32]  # local coordinates\n",
    "    trajectory_sampling: TrajectorySampling = TrajectorySampling(time_horizon=4, interval_length=0.5)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.poses.ndim == 2, \"Trajectory poses should have two dimensions for samples and poses.\"\n",
    "        assert (\n",
    "            self.poses.shape[0] == self.trajectory_sampling.num_poses\n",
    "        ), \"Trajectory poses and sampling have unequal number of poses.\"\n",
    "        assert self.poses.shape[1] == 3, \"Trajectory requires (x, y, heading) at last dim.\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SceneMetadata:\n",
    "    \"\"\"Dataclass of scene metadata (e.g. location) per scene.\"\"\"\n",
    "\n",
    "    log_name: str\n",
    "    scene_token: str\n",
    "    map_name: str\n",
    "    initial_token: str\n",
    "\n",
    "    num_history_frames: int\n",
    "    num_future_frames: int\n",
    "\n",
    "    #  maps between synthetic scenes and the corresponding original scene\n",
    "    #  with the same timestamp in the same log.\n",
    "    #  NOTE: this is not the corresponding first stage scene token\n",
    "    #  for original scenes this is None\n",
    "    corresponding_original_scene: str = None\n",
    "\n",
    "    # maps to the initial frame token (at 0.0s) of the corresponding original scene\n",
    "    # for original scenes this is None\n",
    "    corresponding_original_initial_token: str = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Frame:\n",
    "    \"\"\"Frame dataclass with privileged information.\"\"\"\n",
    "\n",
    "    token: str\n",
    "    timestamp: int\n",
    "    roadblock_ids: List[str]\n",
    "    traffic_lights: List[Tuple[str, bool]]\n",
    "    annotations: Annotations\n",
    "\n",
    "    ego_status: EgoStatus\n",
    "    lidar: Lidar\n",
    "    cameras: Cameras\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Scene:\n",
    "    \"\"\"Scene dataclass defining a single sample in NAVSIM.\"\"\"\n",
    "\n",
    "    # Ground truth information\n",
    "    scene_metadata: SceneMetadata\n",
    "    map_api: AbstractMap\n",
    "    frames: List[Frame]\n",
    "    extended_traffic_light_data: Optional[List[TrafficLightStatuses]] = None\n",
    "    extended_detections_tracks: Optional[List[DetectionsTracks]] = None\n",
    "    \"\"\"\n",
    "    scene_metadata (SceneMetadata): Metadata describing the scene, including its unique identifiers and attributes.\n",
    "    map_api (AbstractMap): Map API interface providing access to map-related information such as lane geometry and topology.\n",
    "    frames (List[Frame]): A sequence of frames describing the state of the ego-vehicle and its surroundings.\n",
    "    extended_traffic_light_data (Optional[List[TrafficLightStatuses]], optional):\n",
    "        A list containing traffic light status information for each future frame after the scene ends.\n",
    "        Each `TrafficLightStatuses` entry includes a `TrafficLightStatusData` object for every lane connector\n",
    "        controlled by a traffic light. Defaults to None.\n",
    "    extended_detections_tracks (Optional[List[DetectionsTracks]], optional):\n",
    "        A list containing detection tracks for each future frame after the scene ends.\n",
    "        This can be used to provide future detections of pedestrians and objects in synthetic scenarios\n",
    "        where future frames are unavailable. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_future_trajectory(self, num_trajectory_frames: Optional[int] = None) -> Trajectory:\n",
    "        \"\"\"\n",
    "        Extracts future trajectory of the human operator in local coordinates (ie. ego rear-axle).\n",
    "        :param num_trajectory_frames: optional number frames to extract poses, defaults to None\n",
    "        :return: trajectory dataclass\n",
    "        \"\"\"\n",
    "\n",
    "        if num_trajectory_frames is None:\n",
    "            num_trajectory_frames = self.scene_metadata.num_future_frames\n",
    "\n",
    "        start_frame_idx = self.scene_metadata.num_history_frames - 1\n",
    "\n",
    "        global_ego_poses = []\n",
    "        for frame_idx in range(start_frame_idx, start_frame_idx + num_trajectory_frames + 1):\n",
    "            global_ego_poses.append(self.frames[frame_idx].ego_status.ego_pose)\n",
    "\n",
    "        local_ego_poses = convert_absolute_to_relative_se2_array(\n",
    "            StateSE2(*global_ego_poses[0]),\n",
    "            np.array(global_ego_poses[1:], dtype=np.float64),\n",
    "        )\n",
    "\n",
    "        return Trajectory(\n",
    "            local_ego_poses,\n",
    "            TrajectorySampling(\n",
    "                num_poses=len(local_ego_poses),\n",
    "                interval_length=NAVSIM_INTERVAL_LENGTH,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def get_history_trajectory(self, num_trajectory_frames: Optional[int] = None) -> Trajectory:\n",
    "        \"\"\"\n",
    "        Extracts past trajectory of ego vehicles in local coordinates (ie. ego rear-axle).\n",
    "        :param num_trajectory_frames: optional number frames to extract poses, defaults to None\n",
    "        :return: trajectory dataclass\n",
    "        \"\"\"\n",
    "\n",
    "        if num_trajectory_frames is None:\n",
    "            num_trajectory_frames = self.scene_metadata.num_history_frames\n",
    "\n",
    "        global_ego_poses = []\n",
    "        for frame_idx in range(num_trajectory_frames):\n",
    "            global_ego_poses.append(self.frames[frame_idx].ego_status.ego_pose)\n",
    "\n",
    "        origin = StateSE2(*global_ego_poses[-1])\n",
    "        local_ego_poses = convert_absolute_to_relative_se2_array(origin, np.array(global_ego_poses, dtype=np.float64))\n",
    "\n",
    "        return Trajectory(\n",
    "            local_ego_poses,\n",
    "            TrajectorySampling(\n",
    "                num_poses=len(local_ego_poses),\n",
    "                interval_length=NAVSIM_INTERVAL_LENGTH,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def get_agent_input(self) -> AgentInput:\n",
    "        \"\"\"\n",
    "        Extracts agents input dataclass (without privileged information) from scene.\n",
    "        :return: agent input dataclass\n",
    "        \"\"\"\n",
    "\n",
    "        local_ego_poses = self.get_history_trajectory().poses\n",
    "        ego_statuses: List[EgoStatus] = []\n",
    "        cameras: List[Cameras] = []\n",
    "        lidars: List[Lidar] = []\n",
    "\n",
    "        for frame_idx in range(self.scene_metadata.num_history_frames):\n",
    "            frame_ego_status = self.frames[frame_idx].ego_status\n",
    "\n",
    "            ego_statuses.append(\n",
    "                EgoStatus(\n",
    "                    ego_pose=local_ego_poses[frame_idx],\n",
    "                    ego_velocity=frame_ego_status.ego_velocity,\n",
    "                    ego_acceleration=frame_ego_status.ego_acceleration,\n",
    "                    driving_command=frame_ego_status.driving_command,\n",
    "                )\n",
    "            )\n",
    "            cameras.append(self.frames[frame_idx].cameras)\n",
    "            lidars.append(self.frames[frame_idx].lidar)\n",
    "\n",
    "        return AgentInput(ego_statuses, cameras, lidars)\n",
    "\n",
    "    @classmethod\n",
    "    def _build_map_api(cls, map_name: str) -> AbstractMap:\n",
    "        \"\"\"Helper classmethod to load map api from name.\"\"\"\n",
    "        assert map_name in MAP_LOCATIONS, f\"The map name {map_name} is invalid, must be in {MAP_LOCATIONS}\"\n",
    "        return get_maps_api(os.getenv(\"NUPLAN_MAPS_ROOT\"), \"nuplan-maps-v1.0\", map_name)\n",
    "\n",
    "    @classmethod\n",
    "    def _build_annotations(cls, scene_frame: Dict) -> Annotations:\n",
    "        \"\"\"Helper classmethod to load annotation dataclass from logs.\"\"\"\n",
    "        return Annotations(\n",
    "            boxes=scene_frame[\"anns\"][\"gt_boxes\"],\n",
    "            names=scene_frame[\"anns\"][\"gt_names\"],\n",
    "            velocity_3d=scene_frame[\"anns\"][\"gt_velocity_3d\"],\n",
    "            instance_tokens=scene_frame[\"anns\"][\"instance_tokens\"],\n",
    "            track_tokens=scene_frame[\"anns\"][\"track_tokens\"],\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def _build_ego_status(cls, scene_frame: Dict) -> EgoStatus:\n",
    "        \"\"\"Helper classmethod to load ego status dataclass from logs.\"\"\"\n",
    "        ego_translation = scene_frame[\"ego2global_translation\"]\n",
    "        ego_quaternion = Quaternion(*scene_frame[\"ego2global_rotation\"])\n",
    "        global_ego_pose = np.array(\n",
    "            [ego_translation[0], ego_translation[1], ego_quaternion.yaw_pitch_roll[0]],\n",
    "            dtype=np.float64,\n",
    "        )\n",
    "        ego_dynamic_state = scene_frame[\"ego_dynamic_state\"]\n",
    "        return EgoStatus(\n",
    "            ego_pose=global_ego_pose,\n",
    "            ego_velocity=np.array(ego_dynamic_state[:2], dtype=np.float32),\n",
    "            ego_acceleration=np.array(ego_dynamic_state[2:], dtype=np.float32),\n",
    "            driving_command=scene_frame[\"driving_command\"],\n",
    "            in_global_frame=True,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_scene_dict_list(\n",
    "        cls,\n",
    "        scene_dict_list: List[Dict],\n",
    "        sensor_blobs_path: Path,\n",
    "        num_history_frames: int,\n",
    "        num_future_frames: int,\n",
    "        sensor_config: SensorConfig,\n",
    "    ) -> Scene:\n",
    "        \"\"\"\n",
    "        Load scene dataclass from scene dictionary list (for log loading).\n",
    "        :param scene_dict_list: list of scene frames (in logs)\n",
    "        :param sensor_blobs_path: root directory of sensor data\n",
    "        :param num_history_frames: number of past and current frames to load\n",
    "        :param num_future_frames: number of future frames to load\n",
    "        :param sensor_config: sensor config dataclass\n",
    "        :return: scene dataclass\n",
    "        \"\"\"\n",
    "        assert len(scene_dict_list) >= 0, \"Scene list is empty!\"\n",
    "        scene_metadata = SceneMetadata(\n",
    "            log_name=scene_dict_list[num_history_frames - 1][\"log_name\"],\n",
    "            scene_token=scene_dict_list[num_history_frames - 1][\"scene_token\"],\n",
    "            map_name=scene_dict_list[num_history_frames - 1][\"map_location\"],\n",
    "            initial_token=scene_dict_list[num_history_frames - 1][\"token\"],\n",
    "            num_history_frames=num_history_frames,\n",
    "            num_future_frames=num_future_frames,\n",
    "        )\n",
    "        map_api = cls._build_map_api(scene_metadata.map_name)\n",
    "\n",
    "        frames: List[Frame] = []\n",
    "        for frame_idx in range(len(scene_dict_list)):\n",
    "            global_ego_status = cls._build_ego_status(scene_dict_list[frame_idx])\n",
    "            annotations = cls._build_annotations(scene_dict_list[frame_idx])\n",
    "\n",
    "            sensor_names = sensor_config.get_sensors_at_iteration(frame_idx)\n",
    "\n",
    "            cameras = Cameras.from_camera_dict(\n",
    "                sensor_blobs_path=sensor_blobs_path,\n",
    "                camera_dict=scene_dict_list[frame_idx][\"cams\"],\n",
    "                sensor_names=sensor_names,\n",
    "            )\n",
    "\n",
    "            lidar = Lidar.from_paths(\n",
    "                sensor_blobs_path=sensor_blobs_path,\n",
    "                lidar_path=Path(scene_dict_list[frame_idx][\"lidar_path\"]),\n",
    "                sensor_names=sensor_names,\n",
    "            )\n",
    "\n",
    "            frame = Frame(\n",
    "                token=scene_dict_list[frame_idx][\"token\"],\n",
    "                timestamp=scene_dict_list[frame_idx][\"timestamp\"],\n",
    "                roadblock_ids=scene_dict_list[frame_idx][\"roadblock_ids\"],\n",
    "                traffic_lights=scene_dict_list[frame_idx][\"traffic_lights\"],\n",
    "                annotations=annotations,\n",
    "                ego_status=global_ego_status,\n",
    "                lidar=lidar,\n",
    "                cameras=cameras,\n",
    "            )\n",
    "            frames.append(frame)\n",
    "\n",
    "        return Scene(scene_metadata=scene_metadata, map_api=map_api, frames=frames)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_scene_dict_list_private(\n",
    "        cls,\n",
    "        scene_dict_list: List[Dict],\n",
    "        sensor_blobs_path: Path,\n",
    "        num_history_frames: int,\n",
    "        num_future_frames: int,\n",
    "        sensor_config: SensorConfig,\n",
    "    ) -> Scene:\n",
    "        \"\"\"\n",
    "        Load scene dataclass from scene dictionary list (for log loading).\n",
    "        :param scene_dict_list: list of scene frames (in logs)\n",
    "        :param sensor_blobs_path: root directory of sensor data\n",
    "        :param num_history_frames: number of past and current frames to load\n",
    "        :param num_future_frames: number of future frames to load\n",
    "        :param sensor_config: sensor config dataclass\n",
    "        :return: scene dataclass\n",
    "        \"\"\"\n",
    "        assert len(scene_dict_list) >= 0, \"Scene list is empty!\"\n",
    "        scene_metadata = SceneMetadata(\n",
    "            log_name=scene_dict_list[num_history_frames - 1][\"log_name\"],\n",
    "            scene_token=scene_dict_list[num_history_frames - 1][\"scene_token\"],\n",
    "            map_name=scene_dict_list[num_history_frames - 1][\"map_location\"],\n",
    "            initial_token=scene_dict_list[num_history_frames - 1][\"token\"],\n",
    "            num_history_frames=num_history_frames,\n",
    "            num_future_frames=num_future_frames,\n",
    "        )\n",
    "\n",
    "        global_ego_poses = []\n",
    "        for frame_idx in range(num_history_frames):\n",
    "            ego_translation = scene_dict_list[frame_idx][\"ego2global_translation\"]\n",
    "            ego_quaternion = Quaternion(*scene_dict_list[frame_idx][\"ego2global_rotation\"])\n",
    "            global_ego_pose = np.array(\n",
    "                [\n",
    "                    ego_translation[0],\n",
    "                    ego_translation[1],\n",
    "                    ego_quaternion.yaw_pitch_roll[0],\n",
    "                ],\n",
    "                dtype=np.float64,\n",
    "            )\n",
    "            global_ego_poses.append(global_ego_pose)\n",
    "\n",
    "        local_ego_poses = convert_absolute_to_relative_se2_array(\n",
    "            StateSE2(*global_ego_poses[-1]),\n",
    "            np.array(global_ego_poses, dtype=np.float64),\n",
    "        )\n",
    "\n",
    "        frames: List[Frame] = []\n",
    "        for frame_idx in range(len(scene_dict_list)):\n",
    "            ego_dynamic_state = scene_dict_list[frame_idx][\"ego_dynamic_state\"]\n",
    "            ego_status = EgoStatus(\n",
    "                ego_pose=np.array(local_ego_poses[frame_idx], dtype=np.float32),\n",
    "                ego_velocity=np.array(ego_dynamic_state[:2], dtype=np.float32),\n",
    "                ego_acceleration=np.array(ego_dynamic_state[2:], dtype=np.float32),\n",
    "                driving_command=scene_dict_list[frame_idx][\"driving_command\"],\n",
    "            )\n",
    "\n",
    "            sensor_names = sensor_config.get_sensors_at_iteration(frame_idx)\n",
    "            cameras = Cameras.from_camera_dict(\n",
    "                sensor_blobs_path=sensor_blobs_path,\n",
    "                camera_dict=scene_dict_list[frame_idx][\"cams\"],\n",
    "                sensor_names=sensor_names,\n",
    "            )\n",
    "\n",
    "            frame = Frame(\n",
    "                token=scene_dict_list[frame_idx][\"token\"],\n",
    "                timestamp=scene_dict_list[frame_idx][\"timestamp\"],\n",
    "                roadblock_ids=scene_dict_list[frame_idx][\"roadblock_ids\"],\n",
    "                traffic_lights=scene_dict_list[frame_idx][\"traffic_lights\"],\n",
    "                annotations=None,\n",
    "                ego_status=ego_status,\n",
    "                lidar=None,\n",
    "                cameras=cameras,\n",
    "            )\n",
    "            frames.append(frame)\n",
    "            \n",
    "        return Scene(scene_metadata=scene_metadata, map_api=None, frames=frames)\n",
    "\n",
    "    def save_to_disk(self, data_path: Path):\n",
    "        \"\"\"\n",
    "        Save scene dataclass to disk.\n",
    "        Note: this will NOT save the images or point clouds.\n",
    "        :param data_path: root directory to save scene data\n",
    "        :param sensor_blobs_path: root directory to sensor data\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.scene_metadata.scene_token is not None, \"Scene token cannot be 'None', when saving to disk.\"\n",
    "        assert data_path.is_dir(), f\"Data path {data_path} is not a directory.\"\n",
    "\n",
    "        # collect all the relevant data for the frames\n",
    "        frames_data = []\n",
    "        for frame in self.frames:\n",
    "            camera_dict = {}\n",
    "            for camera_field in fields(frame.cameras):\n",
    "                camera_name = camera_field.name\n",
    "                camera: Camera = getattr(frame.cameras, camera_name)\n",
    "                if camera.image is not None:\n",
    "                    camera_dict[camera_name] = {\n",
    "                        \"data_path\": camera.camera_path,\n",
    "                        \"sensor2lidar_rotation\": camera.sensor2lidar_rotation,\n",
    "                        \"sensor2lidar_translation\": camera.sensor2lidar_translation,\n",
    "                        \"cam_intrinsic\": camera.intrinsics,\n",
    "                        \"distortion\": camera.distortion,\n",
    "                    }\n",
    "                else:\n",
    "                    camera_dict[camera_name] = {}\n",
    "\n",
    "            if frame.lidar.lidar_pc is not None:\n",
    "                lidar_path = frame.lidar.lidar_path\n",
    "            else:\n",
    "                lidar_path = None\n",
    "\n",
    "            frames_data.append(\n",
    "                {\n",
    "                    \"token\": frame.token,\n",
    "                    \"timestamp\": frame.timestamp,\n",
    "                    \"roadblock_ids\": frame.roadblock_ids,\n",
    "                    \"traffic_lights\": frame.traffic_lights,\n",
    "                    \"annotations\": asdict(frame.annotations),\n",
    "                    \"ego_status\": asdict(frame.ego_status),\n",
    "                    \"lidar_path\": lidar_path,\n",
    "                    \"camera_dict\": camera_dict,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # collect all the relevant data for the scene\n",
    "        scene_dict = {\n",
    "            \"scene_metadata\": asdict(self.scene_metadata),\n",
    "            \"frames\": frames_data,\n",
    "            \"extended_traffic_light_data\": self.extended_traffic_light_data,\n",
    "            \"extended_detections_tracks\": self.extended_detections_tracks,\n",
    "        }\n",
    "\n",
    "        # save the scene_dict to disk\n",
    "        save_path = data_path / f\"{self.scene_metadata.scene_token}.pkl\"\n",
    "\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            pickle.dump(scene_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_disk(\n",
    "        cls,\n",
    "        file_path: Path,\n",
    "        sensor_blobs_path: Path,\n",
    "        sensor_config: SensorConfig = None,\n",
    "    ) -> Scene:\n",
    "        \"\"\"\n",
    "        Load scene dataclass from disk. Only used for synthesized views.\n",
    "        Regular scenes are loaded from logs.\n",
    "        :return: scene dataclass\n",
    "        \"\"\"\n",
    "        if sensor_config is None:\n",
    "            sensor_config = SensorConfig.build_no_sensors()\n",
    "        # Load the metadata\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            scene_data = pickle.load(f)\n",
    "\n",
    "        scene_metadata = SceneMetadata(**scene_data[\"scene_metadata\"])\n",
    "        # build the map from the map_path\n",
    "        map_api = cls._build_map_api(scene_metadata.map_name)\n",
    "\n",
    "        scene_frames: List[Frame] = []\n",
    "        for frame_idx, frame_data in enumerate(scene_data[\"frames\"]):\n",
    "            sensor_names = sensor_config.get_sensors_at_iteration(frame_idx)\n",
    "            lidar_path = Path(frame_data[\"lidar_path\"]) if frame_data[\"lidar_path\"] else None\n",
    "            lidar = Lidar.from_paths(\n",
    "                sensor_blobs_path=sensor_blobs_path,\n",
    "                lidar_path=lidar_path,\n",
    "                sensor_names=sensor_names,\n",
    "            )\n",
    "\n",
    "            cameras = Cameras.from_camera_dict(\n",
    "                sensor_blobs_path=sensor_blobs_path,\n",
    "                camera_dict=frame_data[\"camera_dict\"],\n",
    "                sensor_names=sensor_names,\n",
    "            )\n",
    "\n",
    "            scene_frames.append(\n",
    "                Frame(\n",
    "                    token=frame_data[\"token\"],\n",
    "                    timestamp=frame_data[\"timestamp\"],\n",
    "                    roadblock_ids=frame_data[\"roadblock_ids\"],\n",
    "                    traffic_lights=frame_data[\"traffic_lights\"],\n",
    "                    annotations=Annotations(**frame_data[\"annotations\"]),\n",
    "                    ego_status=EgoStatus(**frame_data[\"ego_status\"]),\n",
    "                    lidar=lidar,\n",
    "                    cameras=cameras,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return Scene(\n",
    "            scene_metadata=scene_metadata,\n",
    "            map_api=map_api,\n",
    "            frames=scene_frames,\n",
    "            extended_traffic_light_data=scene_data[\"extended_traffic_light_data\"],\n",
    "            extended_detections_tracks=scene_data[\"extended_detections_tracks\"],\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SceneFilter:\n",
    "    \"\"\"Scene filtering configuration for scene loading.\"\"\"\n",
    "\n",
    "    num_history_frames: int = 4\n",
    "    num_future_frames: int = 10\n",
    "    frame_interval: Optional[int] = None\n",
    "    has_route: bool = True\n",
    "\n",
    "    max_scenes: Optional[int] = None\n",
    "    log_names: Optional[List[str]] = None\n",
    "    tokens: Optional[List[str]] = None\n",
    "    include_synthetic_scenes: bool = False\n",
    "    all_mapping: Optional[Dict[Tuple[str, str], List[Tuple[str, str]]]] = None\n",
    "    synthetic_scene_tokens: Optional[List[str]] = None\n",
    "\n",
    "    # for reactive and non_reactive\n",
    "    reactive_synthetic_initial_tokens: Optional[List[str]] = None\n",
    "    non_reactive_synthetic_initial_tokens: Optional[List[str]] = None\n",
    "\n",
    "    # TODO: expand filter options\n",
    "\n",
    "    def __post_init__(self):\n",
    "\n",
    "        if self.frame_interval is None:\n",
    "            self.frame_interval = self.num_frames\n",
    "\n",
    "        assert self.num_history_frames >= 1, \"SceneFilter: num_history_frames must greater equal one.\"\n",
    "        assert self.num_future_frames >= 0, \"SceneFilter: num_future_frames must greater equal zero.\"\n",
    "        assert self.frame_interval >= 1, \"SceneFilter: frame_interval must greater equal one.\"\n",
    "\n",
    "        if (\n",
    "            not self.include_synthetic_scenes\n",
    "            and self.synthetic_scene_tokens is not None\n",
    "            and len(self.synthetic_scene_tokens) > 0\n",
    "        ):\n",
    "            warnings.warn(\n",
    "                \"SceneFilter: synthetic_scene_tokens are provided but include_synthetic_scenes is False. No synthetic scenes will be loaded.\"\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def num_frames(self) -> int:\n",
    "        \"\"\"\n",
    "        :return: total number for frames for scenes to extract.\n",
    "        \"\"\"\n",
    "        return self.num_history_frames + self.num_future_frames\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SensorConfig:\n",
    "    \"\"\"Configuration dataclass of agent sensors for memory management.\"\"\"\n",
    "\n",
    "    # Config values of sensors are either\n",
    "    # - bool: Whether to load history or not\n",
    "    # - List[int]: For loading specific history steps\n",
    "    cam_f0: Union[bool, List[int]]\n",
    "    cam_l0: Union[bool, List[int]]\n",
    "    cam_l1: Union[bool, List[int]]\n",
    "    cam_l2: Union[bool, List[int]]\n",
    "    cam_r0: Union[bool, List[int]]\n",
    "    cam_r1: Union[bool, List[int]]\n",
    "    cam_r2: Union[bool, List[int]]\n",
    "    cam_b0: Union[bool, List[int]]\n",
    "    lidar_pc: Union[bool, List[int]]\n",
    "\n",
    "    def get_sensors_at_iteration(self, iteration: int) -> List[str]:\n",
    "        \"\"\"\n",
    "        Creates a list of sensor identifiers given iteration.\n",
    "        :param iteration: integer indicating the history iteration.\n",
    "        :return: list of sensor identifiers to load.\n",
    "        \"\"\"\n",
    "        sensors_at_iteration: List[str] = []\n",
    "        for sensor_name, sensor_include in asdict(self).items():\n",
    "            if isinstance(sensor_include, bool) and sensor_include:\n",
    "                sensors_at_iteration.append(sensor_name)\n",
    "            elif isinstance(sensor_include, list) and iteration in sensor_include:\n",
    "                sensors_at_iteration.append(sensor_name)\n",
    "        return sensors_at_iteration\n",
    "\n",
    "    @classmethod\n",
    "    def build_all_sensors(cls, include: Union[bool, List[int]] = True) -> SensorConfig:\n",
    "        \"\"\"\n",
    "        Classmethod to load all sensors with the same specification.\n",
    "        :param include: boolean or integers for sensors to include, defaults to True\n",
    "        :return: sensor configuration dataclass\n",
    "        \"\"\"\n",
    "        return SensorConfig(\n",
    "            cam_f0=include,\n",
    "            cam_l0=include,\n",
    "            cam_l1=include,\n",
    "            cam_l2=include,\n",
    "            cam_r0=include,\n",
    "            cam_r1=include,\n",
    "            cam_r2=include,\n",
    "            cam_b0=include,\n",
    "            lidar_pc=include,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def build_no_sensors(cls) -> SensorConfig:\n",
    "        \"\"\"\n",
    "        Classmethod to load no sensors.\n",
    "        :return: sensor configuration dataclass\n",
    "        \"\"\"\n",
    "        return cls.build_all_sensors(include=False)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PDMResults:\n",
    "    \"\"\"Helper dataclass to record PDM results.\"\"\"\n",
    "\n",
    "    no_at_fault_collisions: float\n",
    "    drivable_area_compliance: float\n",
    "    driving_direction_compliance: float\n",
    "    traffic_light_compliance: float\n",
    "\n",
    "    ego_progress: float\n",
    "    time_to_collision_within_bound: float\n",
    "    lane_keeping: float\n",
    "    history_comfort: float\n",
    "\n",
    "    multiplicative_metrics_prod: float\n",
    "    weighted_metrics: npt.NDArray[np.float64]\n",
    "    weighted_metrics_array: npt.NDArray[np.float64]\n",
    "\n",
    "    pdm_score: float\n",
    "\n",
    "    @classmethod\n",
    "    def get_empty_results(cls) -> PDMResults:\n",
    "        \"\"\"\n",
    "        Returns an instance of the class where all values are NaN.\n",
    "        :return: empty PDM results dataclass.\n",
    "        \"\"\"\n",
    "        return PDMResults(\n",
    "            no_at_fault_collisions=np.nan,\n",
    "            drivable_area_compliance=np.nan,\n",
    "            driving_direction_compliance=np.nan,\n",
    "            traffic_light_compliance=np.nan,\n",
    "            ego_progress=np.nan,\n",
    "            time_to_collision_within_bound=np.nan,\n",
    "            lane_keeping=np.nan,\n",
    "            history_comfort=np.nan,\n",
    "            multiplicative_metrics_prod=np.nan,\n",
    "            weighted_metrics=np.nan,\n",
    "            weighted_metrics_array=np.nan,\n",
    "            pdm_score=np.nan,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216756f9",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c3e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import lzma\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from navsim.common.dataclasses import AgentInput, Scene, SceneFilter, SensorConfig\n",
    "from navsim.planning.metric_caching.metric_cache import MetricCache\n",
    "\n",
    "FrameList = List[Dict[str, Any]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0495e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataloader\n",
    "\n",
    "def filter_scenes(data_path: Path, scene_filter: SceneFilter) -> Tuple[Dict[str, FrameList], List[str]]:\n",
    "    \"\"\"\n",
    "    Load a set of scenes from dataset, while applying scene filter configuration.\n",
    "    :param data_path: root directory of log folder\n",
    "    :param scene_filter: scene filtering configuration class\n",
    "    :return: dictionary of raw logs format, and list of final frame tokens that can be used to filter synthetic scenes\n",
    "    \"\"\"\n",
    "\n",
    "    def split_list(input_list: List[Any], num_frames: int, frame_interval: int) -> List[List[Any]]:\n",
    "        \"\"\"Helper function to split frame list according to sampling specification.\"\"\"\n",
    "        return [input_list[i : i + num_frames] for i in range(0, len(input_list), frame_interval)]\n",
    "\n",
    "    filtered_scenes: Dict[str, Scene] = {}\n",
    "    # keep track of the final frame tokens which refer to the original scene of potential second stage synthetic scenes\n",
    "    final_frame_tokens: List[str] = []\n",
    "    stop_loading: bool = False\n",
    "\n",
    "    # filter logs\n",
    "    log_files = list(data_path.iterdir())\n",
    "    if scene_filter.log_names is not None:\n",
    "        log_files = [log_file for log_file in log_files if log_file.name.replace(\".pkl\", \"\") in scene_filter.log_names]\n",
    "\n",
    "    if scene_filter.tokens is not None:\n",
    "        filter_tokens = True\n",
    "        tokens = set(scene_filter.tokens)\n",
    "    else:\n",
    "        filter_tokens = False\n",
    "\n",
    "    for log_pickle_path in tqdm(log_files, desc=\"Loading logs\"):\n",
    "\n",
    "        scene_dict_list = pickle.load(open(log_pickle_path, \"rb\"))\n",
    "        for frame_list in split_list(scene_dict_list, scene_filter.num_frames, scene_filter.frame_interval):\n",
    "            # Filter scenes which are too short\n",
    "            if len(frame_list) < scene_filter.num_frames:\n",
    "                continue\n",
    "\n",
    "            # Filter scenes with no route\n",
    "            if scene_filter.has_route and len(frame_list[scene_filter.num_history_frames - 1][\"roadblock_ids\"]) == 0:\n",
    "                continue\n",
    "\n",
    "            # Filter by token\n",
    "            token = frame_list[scene_filter.num_history_frames - 1][\"token\"]\n",
    "            if filter_tokens and token not in tokens:\n",
    "                continue\n",
    "\n",
    "            filtered_scenes[token] = frame_list\n",
    "            final_frame_token = frame_list[scene_filter.num_frames - 1][\"token\"]\n",
    "            #  TODO: if num_future_frames > proposal_sampling frames, then the final_frame_token index is wrong\n",
    "            final_frame_tokens.append(final_frame_token)\n",
    "\n",
    "            if (scene_filter.max_scenes is not None) and (len(filtered_scenes) >= scene_filter.max_scenes):\n",
    "                stop_loading = True\n",
    "                break\n",
    "\n",
    "        if stop_loading:\n",
    "            break\n",
    "\n",
    "    return filtered_scenes, final_frame_tokens\n",
    "\n",
    "\n",
    "def filter_synthetic_scenes(\n",
    "    data_path: Path, scene_filter: SceneFilter, stage1_scenes_final_frames_tokens: List[str]\n",
    ") -> Dict[str, Tuple[Path, str]]:\n",
    "    # Load all the synthetic scenes that belong to the original scenes already loaded\n",
    "    loaded_scenes: Dict[str, Tuple[Path, str, int]] = {}\n",
    "    synthetic_scenes_paths = list(data_path.iterdir())\n",
    "\n",
    "    filter_logs = scene_filter.log_names is not None\n",
    "    filter_tokens = scene_filter.synthetic_scene_tokens is not None\n",
    "\n",
    "    for scene_path in tqdm(synthetic_scenes_paths, desc=\"Loading synthetic scenes\"):\n",
    "        synthetic_scene = Scene.load_from_disk(scene_path, None, None)\n",
    "\n",
    "        # if a token is requested specifically, we load it even if it is not related to the original scenes loaded\n",
    "        if filter_tokens and synthetic_scene.scene_metadata.initial_token not in scene_filter.synthetic_scene_tokens:\n",
    "            continue\n",
    "\n",
    "        # filter by log names\n",
    "        log_name = synthetic_scene.scene_metadata.log_name\n",
    "        if filter_logs and log_name not in scene_filter.log_names:\n",
    "            continue\n",
    "\n",
    "        # if we don't filter for tokens explicitly, we load only the synthetic scenes required to run a second stage for the original scenes loaded\n",
    "        if (\n",
    "            not filter_tokens\n",
    "            and synthetic_scene.scene_metadata.corresponding_original_scene not in stage1_scenes_final_frames_tokens\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        loaded_scenes.update({synthetic_scene.scene_metadata.initial_token: [scene_path, log_name]})\n",
    "\n",
    "    return loaded_scenes\n",
    "\n",
    "\n",
    "class SceneLoader:\n",
    "    \"\"\"Simple data loader of scenes from logs.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: Path,\n",
    "        original_sensor_path: Path,\n",
    "        scene_filter: SceneFilter,\n",
    "        synthetic_sensor_path: Path = None,\n",
    "        synthetic_scenes_path: Path = None,\n",
    "        sensor_config: SensorConfig = SensorConfig.build_no_sensors(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the scene data loader.\n",
    "        :param data_path: root directory of log folder\n",
    "        :param synthetic_sensor_path: root directory of sensor  (synthetic)\n",
    "        :param original_sensor_path: root directory of sensor  (original)\n",
    "        :param scene_filter: dataclass for scene filtering specification\n",
    "        :param sensor_config: dataclass for sensor loading specification, defaults to no sensors\n",
    "        \"\"\"\n",
    "\n",
    "        self.scene_frames_dicts, stage1_scenes_final_frames_tokens = filter_scenes(data_path, scene_filter)\n",
    "        self._synthetic_sensor_path = synthetic_sensor_path\n",
    "        self._original_sensor_path = original_sensor_path\n",
    "        self._scene_filter = scene_filter\n",
    "        self._sensor_config = sensor_config\n",
    "\n",
    "        if scene_filter.include_synthetic_scenes:\n",
    "            assert (\n",
    "                synthetic_scenes_path is not None\n",
    "            ), \"Synthetic scenes path cannot be None, when synthetic scenes_filter.include_synthetic_scenes is set to True.\"\n",
    "            self.synthetic_scenes = filter_synthetic_scenes(\n",
    "                data_path=synthetic_scenes_path,\n",
    "                scene_filter=scene_filter,\n",
    "                stage1_scenes_final_frames_tokens=stage1_scenes_final_frames_tokens,\n",
    "            )\n",
    "            self.synthetic_scenes_tokens = set(self.synthetic_scenes.keys())\n",
    "        else:\n",
    "            self.synthetic_scenes = {}\n",
    "            self.synthetic_scenes_tokens = set()\n",
    "\n",
    "    @property\n",
    "    def tokens(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        :return: list of scene identifiers for loading.\n",
    "        \"\"\"\n",
    "        return list(self.scene_frames_dicts.keys()) + list(self.synthetic_scenes.keys())\n",
    "\n",
    "    @property\n",
    "    def tokens_stage_one(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        original scenes\n",
    "        :return: list of scene identifiers for loading.\n",
    "        \"\"\"\n",
    "        return list(self.scene_frames_dicts.keys())\n",
    "\n",
    "    @property\n",
    "    def reactive_tokens_stage_two(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        reactive synthetic scenes\n",
    "        :return: list of scene identifiers for loading.\n",
    "        \"\"\"\n",
    "        reactive_synthetic_initial_tokens = self._scene_filter.reactive_synthetic_initial_tokens\n",
    "        if reactive_synthetic_initial_tokens is None:\n",
    "            return None\n",
    "        return list(set(self.synthetic_scenes_tokens) & set(reactive_synthetic_initial_tokens))\n",
    "\n",
    "    @property\n",
    "    def non_reactive_tokens_stage_two(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        non reactive synthetic scenes\n",
    "        :return: list of scene identifiers for loading.\n",
    "        \"\"\"\n",
    "        non_reactive_synthetic_initial_tokens = self._scene_filter.non_reactive_synthetic_initial_tokens\n",
    "        if non_reactive_synthetic_initial_tokens is None:\n",
    "            return None\n",
    "        return list(set(self.synthetic_scenes_tokens) & set(non_reactive_synthetic_initial_tokens))\n",
    "\n",
    "    @property\n",
    "    def reactive_tokens(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        original scenes and reactive synthetic scenes\n",
    "        :return: list of scene identifiers for loading.\n",
    "        \"\"\"\n",
    "        reactive_synthetic_initial_tokens = self._scene_filter.reactive_synthetic_initial_tokens\n",
    "        if reactive_synthetic_initial_tokens is None:\n",
    "            return list(self.scene_frames_dicts.keys())\n",
    "        return list(self.scene_frames_dicts.keys()) + list(\n",
    "            set(self.synthetic_scenes_tokens) & set(reactive_synthetic_initial_tokens)\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def non_reactive_tokens(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        original scenes and non reactive synthetic scenes\n",
    "        :return: list of scene identifiers for loading.\n",
    "        \"\"\"\n",
    "        non_reactive_synthetic_initial_tokens = self._scene_filter.non_reactive_synthetic_initial_tokens\n",
    "        if non_reactive_synthetic_initial_tokens is None:\n",
    "            return list(self.scene_frames_dicts.keys())\n",
    "        return list(self.scene_frames_dicts.keys()) + list(\n",
    "            set(self.synthetic_scenes_tokens) & set(non_reactive_synthetic_initial_tokens)\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        :return: number for scenes possible to load.\n",
    "        \"\"\"\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx) -> str:\n",
    "        \"\"\"\n",
    "        :param idx: index of scene\n",
    "        :return: unique scene identifier\n",
    "        \"\"\"\n",
    "        return self.tokens[idx]\n",
    "\n",
    "    def get_scene_from_token(self, token: str) -> Scene:\n",
    "        \"\"\"\n",
    "        Loads scene given a scene identifier string (token).\n",
    "        :param token: scene identifier string.\n",
    "        :return: scene dataclass\n",
    "        \"\"\"\n",
    "        assert token in self.tokens\n",
    "        if token in self.synthetic_scenes:\n",
    "            return Scene.load_from_disk(\n",
    "                file_path=self.synthetic_scenes[token][0],\n",
    "                sensor_blobs_path=self._synthetic_sensor_path,\n",
    "                sensor_config=self._sensor_config,\n",
    "            )\n",
    "        else:\n",
    "            return Scene.from_scene_dict_list(\n",
    "                self.scene_frames_dicts[token],\n",
    "                self._original_sensor_path,\n",
    "                num_history_frames=self._scene_filter.num_history_frames,\n",
    "                num_future_frames=self._scene_filter.num_future_frames,\n",
    "                sensor_config=self._sensor_config,\n",
    "            )\n",
    "\n",
    "    def get_agent_input_from_token(self, token: str) -> AgentInput:\n",
    "        \"\"\"\n",
    "        Loads agent input given a scene identifier string (token).\n",
    "        :param token: scene identifier string.\n",
    "        :return: agent input dataclass\n",
    "        \"\"\"\n",
    "        assert token in self.tokens\n",
    "        if token in self.synthetic_scenes:\n",
    "            return Scene.load_from_disk(\n",
    "                file_path=self.synthetic_scenes[token][0],\n",
    "                sensor_blobs_path=self._synthetic_sensor_path,\n",
    "                sensor_config=self._sensor_config,\n",
    "            ).get_agent_input()\n",
    "        else:\n",
    "            return AgentInput.from_scene_dict_list(\n",
    "                self.scene_frames_dicts[token],\n",
    "                self._original_sensor_path,\n",
    "                num_history_frames=self._scene_filter.num_history_frames,\n",
    "                sensor_config=self._sensor_config,\n",
    "            )\n",
    "\n",
    "    def get_tokens_list_per_log(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Collect tokens for each logs file given filtering.\n",
    "        :return: dictionary of logs names and tokens\n",
    "        \"\"\"\n",
    "        # generate a dict that contains a list of tokens for each log-name\n",
    "        tokens_per_logs: Dict[str, List[str]] = {}\n",
    "        for token, scene_dict_list in self.scene_frames_dicts.items():\n",
    "            log_name = scene_dict_list[0][\"log_name\"]\n",
    "            if tokens_per_logs.get(log_name):\n",
    "                tokens_per_logs[log_name].append(token)\n",
    "            else:\n",
    "                tokens_per_logs.update({log_name: [token]})\n",
    "\n",
    "        for scene_path, log_name in self.synthetic_scenes.values():\n",
    "            if tokens_per_logs.get(log_name):\n",
    "                tokens_per_logs[log_name].append(scene_path.stem)\n",
    "            else:\n",
    "                tokens_per_logs.update({log_name: [scene_path.stem]})\n",
    "\n",
    "        return tokens_per_logs\n",
    "\n",
    "\n",
    "class MetricCacheLoader:\n",
    "    \"\"\"Simple dataloader for metric cache.\"\"\"\n",
    "\n",
    "    def __init__(self, cache_path: Path, file_name: str = \"metric_cache.pkl\"):\n",
    "        \"\"\"\n",
    "        Initializes the metric cache loader.\n",
    "        :param cache_path: directory of cache folder\n",
    "        :param file_name: file name of cached files, defaults to \"metric_cache.pkl\"\n",
    "        \"\"\"\n",
    "\n",
    "        self._file_name = file_name\n",
    "        self.metric_cache_paths = self._load_metric_cache_paths(cache_path)\n",
    "\n",
    "    def _load_metric_cache_paths(self, cache_path: Path) -> Dict[str, Path]:\n",
    "        \"\"\"\n",
    "        Helper function to load all cache file paths from folder.\n",
    "        :param cache_path: directory of cache folder\n",
    "        :return: dictionary of token and file path\n",
    "        \"\"\"\n",
    "        metadata_dir = cache_path / \"metadata\"\n",
    "        metadata_file = [file for file in metadata_dir.iterdir() if \".csv\" in str(file)][0]\n",
    "        with open(str(metadata_file), \"r\") as f:\n",
    "            cache_paths = f.read().splitlines()[1:]\n",
    "        metric_cache_dict = {cache_path.split(\"/\")[-2]: cache_path for cache_path in cache_paths}\n",
    "        return metric_cache_dict\n",
    "\n",
    "    @property\n",
    "    def tokens(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        :return: list of scene identifiers for loading.\n",
    "        \"\"\"\n",
    "        return list(self.metric_cache_paths.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        :return: number for scenes possible to load.\n",
    "        \"\"\"\n",
    "        return len(self.metric_cache_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> MetricCache:\n",
    "        \"\"\"\n",
    "        :param idx: index of cache to cache to load\n",
    "        :return: metric cache dataclass\n",
    "        \"\"\"\n",
    "        return self.get_from_token(self.tokens[idx])\n",
    "\n",
    "    def get_from_token(self, token: str) -> MetricCache:\n",
    "        \"\"\"\n",
    "        Load metric cache from scene identifier\n",
    "        :param token: unique identifier of scene\n",
    "        :return: metric cache dataclass\n",
    "        \"\"\"\n",
    "        with lzma.open(self.metric_cache_paths[token], \"rb\") as f:\n",
    "            metric_cache: MetricCache = pickle.load(f)\n",
    "        return metric_cache\n",
    "\n",
    "    def to_pickle(self, path: Path) -> None:\n",
    "        \"\"\"\n",
    "        Dumps complete metric cache into pickle.\n",
    "        :param path: directory of cache folder\n",
    "        \"\"\"\n",
    "        full_metric_cache = {}\n",
    "        for token in tqdm(self.tokens):\n",
    "            full_metric_cache[token] = self.get_from_token(token)\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(full_metric_cache, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6783656",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab338386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfuser Agent \n",
    "\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from navsim.agents.diffusiondrive.transfuser_config import TransfuserConfig\n",
    "from navsim.agents.diffusiondrive.transfuser_backbone import TransfuserBackbone\n",
    "from navsim.agents.diffusiondrive.transfuser_features import BoundingBox2DIndex\n",
    "from navsim.common.enums import StateSE2Index\n",
    "from diffusers.schedulers import DDIMScheduler\n",
    "from navsim.agents.diffusiondrive.modules.conditional_unet1d import ConditionalUnet1D,SinusoidalPosEmb\n",
    "import torch.nn.functional as F\n",
    "from navsim.agents.diffusiondrive.modules.blocks import linear_relu_ln,bias_init_with_prob, gen_sineembed_for_position, GridSampleCrossBEVAttention\n",
    "from navsim.agents.diffusiondrive.modules.multimodal_loss import LossComputer\n",
    "from torch.nn import TransformerDecoder,TransformerDecoderLayer\n",
    "from typing import Any, List, Dict, Optional, Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Head\n",
    "\n",
    "class AgentHead(nn.Module):\n",
    "    \"\"\"Bounding box prediction head.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_agents: int,\n",
    "        d_ffn: int,\n",
    "        d_model: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes prediction head.\n",
    "        :param num_agents: maximum number of agents to predict\n",
    "        :param d_ffn: dimensionality of feed-forward network\n",
    "        :param d_model: input dimensionality\n",
    "        \"\"\"\n",
    "        super(AgentHead, self).__init__()\n",
    "\n",
    "        self._num_objects = num_agents\n",
    "        self._d_model = d_model\n",
    "        self._d_ffn = d_ffn\n",
    "\n",
    "        self._mlp_states = nn.Sequential(\n",
    "            nn.Linear(self._d_model, self._d_ffn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self._d_ffn, BoundingBox2DIndex.size()),\n",
    "        )\n",
    "\n",
    "        self._mlp_label = nn.Sequential(\n",
    "            nn.Linear(self._d_model, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, agent_queries) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Torch module forward pass.\"\"\"\n",
    "\n",
    "        agent_states = self._mlp_states(agent_queries)\n",
    "        agent_states[..., BoundingBox2DIndex.POINT] = agent_states[..., BoundingBox2DIndex.POINT].tanh() * 32\n",
    "        agent_states[..., BoundingBox2DIndex.HEADING] = agent_states[..., BoundingBox2DIndex.HEADING].tanh() * np.pi\n",
    "\n",
    "        agent_labels = self._mlp_label(agent_queries).squeeze(dim=-1)\n",
    "\n",
    "        return {\"agent_states\": agent_states, \"agent_labels\": agent_labels}\n",
    "\n",
    "# Module\n",
    "\n",
    "class DiffMotionPlanningRefinementModule(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dims=256,\n",
    "        ego_fut_ts=8,\n",
    "        ego_fut_mode=20,\n",
    "        if_zeroinit_reg=True,\n",
    "    ):\n",
    "        super(DiffMotionPlanningRefinementModule, self).__init__()\n",
    "        self.embed_dims = embed_dims\n",
    "        self.ego_fut_ts = ego_fut_ts\n",
    "        self.ego_fut_mode = ego_fut_mode\n",
    "        self.plan_cls_branch = nn.Sequential(\n",
    "            *linear_relu_ln(embed_dims, 1, 2),\n",
    "            nn.Linear(embed_dims, 1),\n",
    "        )\n",
    "        self.plan_reg_branch = nn.Sequential(\n",
    "            nn.Linear(embed_dims, embed_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dims, embed_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dims, ego_fut_ts * 3),\n",
    "        )\n",
    "        self.if_zeroinit_reg = False\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        if self.if_zeroinit_reg:\n",
    "            nn.init.constant_(self.plan_reg_branch[-1].weight, 0)\n",
    "            nn.init.constant_(self.plan_reg_branch[-1].bias, 0)\n",
    "\n",
    "        bias_init = bias_init_with_prob(0.01)\n",
    "        nn.init.constant_(self.plan_cls_branch[-1].bias, bias_init)\n",
    "    def forward(\n",
    "        self,\n",
    "        traj_feature,\n",
    "    ):\n",
    "        bs, ego_fut_mode, _ = traj_feature.shape\n",
    "\n",
    "        # 6. get final prediction\n",
    "        traj_feature = traj_feature.view(bs, ego_fut_mode,-1)\n",
    "        plan_cls = self.plan_cls_branch(traj_feature).squeeze(-1)\n",
    "        traj_delta = self.plan_reg_branch(traj_feature)\n",
    "        plan_reg = traj_delta.reshape(bs,ego_fut_mode, self.ego_fut_ts, 3)\n",
    "\n",
    "        return plan_reg, plan_cls\n",
    "    \n",
    "class ModulationLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dims: int, condition_dims: int):\n",
    "        super(ModulationLayer, self).__init__()\n",
    "        self.if_zeroinit_scale=False\n",
    "        self.embed_dims = embed_dims\n",
    "        self.scale_shift_mlp = nn.Sequential(\n",
    "            nn.Mish(),\n",
    "            nn.Linear(condition_dims, embed_dims*2),\n",
    "        )\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        if self.if_zeroinit_scale:\n",
    "            nn.init.constant_(self.scale_shift_mlp[-1].weight, 0)\n",
    "            nn.init.constant_(self.scale_shift_mlp[-1].bias, 0)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        traj_feature,\n",
    "        time_embed,\n",
    "        global_cond=None,\n",
    "        global_img=None,\n",
    "    ):\n",
    "        if global_cond is not None:\n",
    "            global_feature = torch.cat([\n",
    "                    global_cond, time_embed\n",
    "                ], axis=-1)\n",
    "        else:\n",
    "            global_feature = time_embed\n",
    "        if global_img is not None:\n",
    "            global_img = global_img.flatten(2,3).permute(0,2,1).contiguous()\n",
    "            global_feature = torch.cat([\n",
    "                    global_img, global_feature\n",
    "                ], axis=-1)\n",
    "        \n",
    "        scale_shift = self.scale_shift_mlp(global_feature)\n",
    "        scale,shift = scale_shift.chunk(2,dim=-1)\n",
    "        traj_feature = traj_feature * (1 + scale) + shift\n",
    "        return traj_feature\n",
    "\n",
    "# Decoder Layer\n",
    "\n",
    "class CustomTransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_poses,\n",
    "                 d_model,\n",
    "                 d_ffn,\n",
    "                 config,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.cross_bev_attention = GridSampleCrossBEVAttention(\n",
    "            config.tf_d_model,\n",
    "            config.tf_num_head,\n",
    "            num_points=num_poses,\n",
    "            config=config,\n",
    "            in_bev_dims=256,\n",
    "        )\n",
    "        self.cross_agent_attention = nn.MultiheadAttention(\n",
    "            config.tf_d_model,\n",
    "            config.tf_num_head,\n",
    "            dropout=config.tf_dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.cross_ego_attention = nn.MultiheadAttention(\n",
    "            config.tf_d_model,\n",
    "            config.tf_num_head,\n",
    "            dropout=config.tf_dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config.tf_d_model, config.tf_d_ffn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.tf_d_ffn, config.tf_d_model),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(config.tf_d_model)\n",
    "        self.norm2 = nn.LayerNorm(config.tf_d_model)\n",
    "        self.norm3 = nn.LayerNorm(config.tf_d_model)\n",
    "        self.time_modulation = ModulationLayer(config.tf_d_model,256)\n",
    "        self.task_decoder = DiffMotionPlanningRefinementModule(\n",
    "            embed_dims=config.tf_d_model,\n",
    "            ego_fut_ts=num_poses,\n",
    "            ego_fut_mode=20,\n",
    "        )\n",
    "\n",
    "    def forward(self, \n",
    "                traj_feature, \n",
    "                noisy_traj_points, \n",
    "                bev_feature, \n",
    "                bev_spatial_shape, \n",
    "                agents_query, \n",
    "                ego_query, \n",
    "                time_embed, \n",
    "                status_encoding,\n",
    "                global_img=None):\n",
    "        traj_feature = self.cross_bev_attention(traj_feature,noisy_traj_points,bev_feature,bev_spatial_shape)\n",
    "        traj_feature = traj_feature + self.dropout(self.cross_agent_attention(traj_feature, agents_query,agents_query)[0])\n",
    "        traj_feature = self.norm1(traj_feature)\n",
    "        \n",
    "        # traj_feature = traj_feature + self.dropout(self.self_attn(traj_feature, traj_feature, traj_feature)[0])\n",
    "\n",
    "        # 4.5 cross attention with  ego query\n",
    "        traj_feature = traj_feature + self.dropout1(self.cross_ego_attention(traj_feature, ego_query,ego_query)[0])\n",
    "        traj_feature = self.norm2(traj_feature)\n",
    "        \n",
    "        # 4.6 feedforward network\n",
    "        traj_feature = self.norm3(self.ffn(traj_feature))\n",
    "        # 4.8 modulate with time steps\n",
    "        traj_feature = self.time_modulation(traj_feature, time_embed,global_cond=None,global_img=global_img)\n",
    "        \n",
    "        # 4.9 predict the offset & heading\n",
    "        poses_reg, poses_cls = self.task_decoder(traj_feature) #bs,20,8,3; bs,20\n",
    "        poses_reg[...,:2] = poses_reg[...,:2] + noisy_traj_points\n",
    "        poses_reg[..., StateSE2Index.HEADING] = poses_reg[..., StateSE2Index.HEADING].tanh() * np.pi\n",
    "\n",
    "        return poses_reg, poses_cls\n",
    "def _get_clones(module, N):\n",
    "    # FIXME: copy.deepcopy() is not defined on nn.module\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "# Decoder\n",
    "\n",
    "class CustomTransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        decoder_layer, \n",
    "        num_layers,\n",
    "        norm=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        torch._C._log_api_usage_once(f\"torch.nn.modules.{self.__class__.__name__}\")\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self, \n",
    "                traj_feature, \n",
    "                noisy_traj_points, \n",
    "                bev_feature, \n",
    "                bev_spatial_shape, \n",
    "                agents_query, \n",
    "                ego_query, \n",
    "                time_embed, \n",
    "                status_encoding,\n",
    "                global_img=None):\n",
    "        poses_reg_list = []\n",
    "        poses_cls_list = []\n",
    "        traj_points = noisy_traj_points\n",
    "        for mod in self.layers:\n",
    "            poses_reg, poses_cls = mod(traj_feature, traj_points, bev_feature, bev_spatial_shape, agents_query, ego_query, time_embed, status_encoding,global_img)\n",
    "            poses_reg_list.append(poses_reg)\n",
    "            poses_cls_list.append(poses_cls)\n",
    "            traj_points = poses_reg[...,:2].clone().detach()\n",
    "        return poses_reg_list, poses_cls_list\n",
    "\n",
    "\n",
    "class TrajectoryHead(nn.Module):\n",
    "    \"\"\"Trajectory prediction head.\"\"\"\n",
    "\n",
    "    def __init__(self, num_poses: int, d_ffn: int, d_model: int, plan_anchor_path: str,config: TransfuserConfig):\n",
    "        \"\"\"\n",
    "        Initializes trajectory head.\n",
    "        :param num_poses: number of (x,y,) poses to predict\n",
    "        :param d_ffn: dimensionality of feed-forward network\n",
    "        :param d_model: input dimensionality\n",
    "        \"\"\"\n",
    "        super(TrajectoryHead, self).__init__()\n",
    "\n",
    "        self._num_poses = num_poses\n",
    "        self._d_model = d_model\n",
    "        self._d_ffn = d_ffn\n",
    "        self.diff_loss_weight = 2.0\n",
    "        self.ego_fut_mode = 20\n",
    "\n",
    "        self.diffusion_scheduler = DDIMScheduler(\n",
    "            num_train_timesteps=1000,\n",
    "            beta_schedule=\"scaled_linear\",\n",
    "            prediction_type=\"sample\",\n",
    "        )\n",
    "\n",
    "\n",
    "        plan_anchor = np.load(plan_anchor_path)\n",
    "\n",
    "        self.plan_anchor = nn.Parameter(\n",
    "            torch.tensor(plan_anchor, dtype=torch.float32),\n",
    "            requires_grad=False,\n",
    "        ) # 20,8,2\n",
    "        self.plan_anchor_encoder = nn.Sequential(\n",
    "            *linear_relu_ln(d_model, 1, 1,512),\n",
    "            nn.Linear(d_model, d_model),\n",
    "        )\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(d_model),\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "        )\n",
    "\n",
    "        diff_decoder_layer = CustomTransformerDecoderLayer(\n",
    "            num_poses=num_poses,\n",
    "            d_model=d_model,\n",
    "            d_ffn=d_ffn,\n",
    "            config=config,\n",
    "        )\n",
    "        self.diff_decoder = CustomTransformerDecoder(diff_decoder_layer, 2)\n",
    "\n",
    "        self.loss_computer = LossComputer(config)\n",
    "    def norm_odo(self, odo_info_fut):\n",
    "        odo_info_fut_x = odo_info_fut[..., 0:1]\n",
    "        odo_info_fut_y = odo_info_fut[..., 1:2]\n",
    "        odo_info_fut_head = odo_info_fut[..., 2:3]\n",
    "\n",
    "        odo_info_fut_x = 2*(odo_info_fut_x + 1.2)/56.9 -1\n",
    "        odo_info_fut_y = 2*(odo_info_fut_y + 20)/46 -1\n",
    "        odo_info_fut_head = 2*(odo_info_fut_head + 2)/3.9 -1\n",
    "        return torch.cat([odo_info_fut_x, odo_info_fut_y, odo_info_fut_head], dim=-1)\n",
    "    def denorm_odo(self, odo_info_fut):\n",
    "        odo_info_fut_x = odo_info_fut[..., 0:1]\n",
    "        odo_info_fut_y = odo_info_fut[..., 1:2]\n",
    "        odo_info_fut_head = odo_info_fut[..., 2:3]\n",
    "\n",
    "        odo_info_fut_x = (odo_info_fut_x + 1)/2 * 56.9 - 1.2\n",
    "        odo_info_fut_y = (odo_info_fut_y + 1)/2 * 46 - 20\n",
    "        odo_info_fut_head = (odo_info_fut_head + 1)/2 * 3.9 - 2\n",
    "        return torch.cat([odo_info_fut_x, odo_info_fut_y, odo_info_fut_head], dim=-1)\n",
    "    def forward(self, ego_query, agents_query, bev_feature,bev_spatial_shape,status_encoding, targets=None,global_img=None) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Torch module forward pass.\"\"\"\n",
    "        if self.training:\n",
    "            return self.forward_train(ego_query, agents_query, bev_feature,bev_spatial_shape,status_encoding,targets,global_img)\n",
    "        else:\n",
    "            return self.forward_test(ego_query, agents_query, bev_feature,bev_spatial_shape,status_encoding,global_img)\n",
    "\n",
    "\n",
    "    def forward_train(self, ego_query,agents_query,bev_feature,bev_spatial_shape,status_encoding, targets=None,global_img=None) -> Dict[str, torch.Tensor]:\n",
    "        bs = ego_query.shape[0]\n",
    "        device = ego_query.device\n",
    "        # 1. add truncated noise to the plan anchor\n",
    "        plan_anchor = self.plan_anchor.unsqueeze(0).repeat(bs,1,1,1)\n",
    "        odo_info_fut = self.norm_odo(plan_anchor)\n",
    "        timesteps = torch.randint(\n",
    "            0, 50,\n",
    "            (bs,), device=device\n",
    "        )\n",
    "        noise = torch.randn(odo_info_fut.shape, device=device)\n",
    "        noisy_traj_points = self.diffusion_scheduler.add_noise(\n",
    "            original_samples=odo_info_fut,\n",
    "            noise=noise,\n",
    "            timesteps=timesteps,\n",
    "        ).float()\n",
    "        noisy_traj_points = torch.clamp(noisy_traj_points, min=-1, max=1)\n",
    "        noisy_traj_points = self.denorm_odo(noisy_traj_points)\n",
    "\n",
    "        ego_fut_mode = noisy_traj_points.shape[1]\n",
    "        # 2. proj noisy_traj_points to the query\n",
    "        traj_pos_embed = gen_sineembed_for_position(noisy_traj_points,hidden_dim=64)\n",
    "        traj_pos_embed = traj_pos_embed.flatten(-2)\n",
    "        traj_feature = self.plan_anchor_encoder(traj_pos_embed)\n",
    "        traj_feature = traj_feature.view(bs,ego_fut_mode,-1)\n",
    "        # 3. embed the timesteps\n",
    "        time_embed = self.time_mlp(timesteps)\n",
    "        time_embed = time_embed.view(bs,1,-1)\n",
    "\n",
    "\n",
    "        # 4. begin the stacked decoder\n",
    "        poses_reg_list, poses_cls_list = self.diff_decoder(traj_feature, noisy_traj_points, bev_feature, bev_spatial_shape, agents_query, ego_query, time_embed, status_encoding,global_img)\n",
    "\n",
    "        trajectory_loss_dict = {}\n",
    "        ret_traj_loss = 0\n",
    "        for idx, (poses_reg, poses_cls) in enumerate(zip(poses_reg_list, poses_cls_list)):\n",
    "            trajectory_loss = self.loss_computer(poses_reg, poses_cls, targets, plan_anchor)\n",
    "            trajectory_loss_dict[f\"trajectory_loss_{idx}\"] = trajectory_loss\n",
    "            ret_traj_loss += trajectory_loss\n",
    "\n",
    "        mode_idx = poses_cls_list[-1].argmax(dim=-1)\n",
    "        mode_idx = mode_idx[...,None,None,None].repeat(1,1,self._num_poses,3)\n",
    "        best_reg = torch.gather(poses_reg_list[-1], 1, mode_idx).squeeze(1)\n",
    "        return {\"trajectory\": best_reg,\"trajectory_loss\":ret_traj_loss,\"trajectory_loss_dict\":trajectory_loss_dict}\n",
    "\n",
    "    def forward_test(self, ego_query,agents_query,bev_feature,bev_spatial_shape,status_encoding,global_img) -> Dict[str, torch.Tensor]:\n",
    "        step_num = 2\n",
    "        bs = ego_query.shape[0]\n",
    "        device = ego_query.device\n",
    "        self.diffusion_scheduler.set_timesteps(1000, device)\n",
    "        step_ratio = 20 / step_num\n",
    "        roll_timesteps = (np.arange(0, step_num) * step_ratio).round()[::-1].copy().astype(np.int64)\n",
    "        roll_timesteps = torch.from_numpy(roll_timesteps).to(device)\n",
    "\n",
    "\n",
    "        # 1. add truncated noise to the plan anchor\n",
    "        plan_anchor = self.plan_anchor.unsqueeze(0).repeat(bs,1,1,1)\n",
    "        img = self.norm_odo(plan_anchor)\n",
    "        noise = torch.randn(img.shape, device=device)\n",
    "        trunc_timesteps = torch.ones((bs,), device=device, dtype=torch.long) * 8\n",
    "        img = self.diffusion_scheduler.add_noise(original_samples=img, noise=noise, timesteps=trunc_timesteps)\n",
    "        noisy_trajs = self.denorm_odo(img)\n",
    "        ego_fut_mode = img.shape[1]\n",
    "        for k in roll_timesteps[:]:\n",
    "            x_boxes = torch.clamp(img, min=-1, max=1)\n",
    "            noisy_traj_points = self.denorm_odo(x_boxes)\n",
    "\n",
    "            # 2. proj noisy_traj_points to the query\n",
    "            traj_pos_embed = gen_sineembed_for_position(noisy_traj_points,hidden_dim=64)\n",
    "            traj_pos_embed = traj_pos_embed.flatten(-2)\n",
    "            traj_feature = self.plan_anchor_encoder(traj_pos_embed)\n",
    "            traj_feature = traj_feature.view(bs,ego_fut_mode,-1)\n",
    "\n",
    "            timesteps = k\n",
    "            if not torch.is_tensor(timesteps):\n",
    "                # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "                timesteps = torch.tensor([timesteps], dtype=torch.long, device=img.device)\n",
    "            elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "                timesteps = timesteps[None].to(img.device)\n",
    "            \n",
    "            # 3. embed the timesteps\n",
    "            timesteps = timesteps.expand(img.shape[0])\n",
    "            time_embed = self.time_mlp(timesteps)\n",
    "            time_embed = time_embed.view(bs,1,-1)\n",
    "\n",
    "            # 4. begin the stacked decoder\n",
    "            poses_reg_list, poses_cls_list = self.diff_decoder(traj_feature, noisy_traj_points, bev_feature, bev_spatial_shape, agents_query, ego_query, time_embed, status_encoding,global_img)\n",
    "            poses_reg = poses_reg_list[-1]\n",
    "            poses_cls = poses_cls_list[-1]\n",
    "            x_start = poses_reg[...,:2]\n",
    "            x_start = self.norm_odo(x_start)\n",
    "            img = self.diffusion_scheduler.step(\n",
    "                model_output=x_start,\n",
    "                timestep=k,\n",
    "                sample=img\n",
    "            ).prev_sample\n",
    "        mode_idx = poses_cls.argmax(dim=-1)\n",
    "        mode_idx = mode_idx[...,None,None,None].repeat(1,1,self._num_poses,3)\n",
    "        best_reg = torch.gather(poses_reg, 1, mode_idx).squeeze(1)\n",
    "        return {\"trajectory\": best_reg}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca6161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "config=TransfuserConfig()\n",
    "query_splits = [\n",
    "    1,\n",
    "    config.num_bounding_boxes,\n",
    "]\n",
    "\n",
    "# Backbone\n",
    "backbone = TransfuserBackbone(config=config)\n",
    "\n",
    "# Embedding layer\n",
    "keyval_embedding = nn.Embedding(8**2 + 1, config.tf_d_model)  # 8x8 feature grid + trajectory\n",
    "print(f\"keyval_embedding = {keyval_embedding}\")\n",
    "query_embedding = nn.Embedding(sum(query_splits), config.tf_d_model)\n",
    "print(f\"query_embedding = {query_embedding}\")\n",
    "\n",
    "\n",
    "# BEV features \n",
    "bev_downscale = nn.Conv2d(512, config.tf_d_model, kernel_size=1)\n",
    "status_encoding = nn.Linear(4 + 2 + 2, config.tf_d_model)\n",
    "\n",
    "bev_semantic_head = nn.Sequential(\n",
    "    nn.Conv2d(\n",
    "        config.bev_features_channels,\n",
    "        config.bev_features_channels,\n",
    "        kernel_size=(3, 3),\n",
    "        stride=1,\n",
    "        padding=(1, 1),\n",
    "        bias=True,\n",
    "    ),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(\n",
    "        config.bev_features_channels,\n",
    "        config.num_bev_classes,\n",
    "        kernel_size=(1, 1),\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        bias=True,\n",
    "    ),\n",
    "    nn.Upsample(\n",
    "        size=(config.lidar_resolution_height // 2, config.lidar_resolution_width),\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "# Decoder\n",
    "\n",
    "tf_decoder_layer = nn.TransformerDecoderLayer(\n",
    "    d_model=config.tf_d_model,\n",
    "    nhead=config.tf_num_head,\n",
    "    dim_feedforward=config.tf_d_ffn,\n",
    "    dropout=config.tf_dropout,\n",
    "    batch_first=True,\n",
    ")\n",
    "\n",
    "tf_decoder = nn.TransformerDecoder(tf_decoder_layer, config.tf_num_layers)\n",
    "\n",
    "# Agent Head\n",
    "agent_head = AgentHead(\n",
    "    num_agents=config.num_bounding_boxes,\n",
    "    d_ffn=config.tf_d_ffn,\n",
    "    d_model=config.tf_d_model,\n",
    ")\n",
    "\n",
    "# Trajectory Head\n",
    "trajectory_head = TrajectoryHead(\n",
    "    num_poses=config.trajectory_sampling.num_poses,\n",
    "    d_ffn=config.tf_d_ffn,\n",
    "    d_model=config.tf_d_model,\n",
    "    plan_anchor_path=config.plan_anchor_path,\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "# BEV Projection\n",
    "bev_proj = nn.Sequential(\n",
    "    *linear_relu_ln(256, 1, 1,320),\n",
    ")\n",
    "print(f\"BEV Proj: {bev_proj}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8d9c40",
   "metadata": {},
   "source": [
    "# Scene Loader / Feature Builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207bcb9d",
   "metadata": {},
   "source": [
    "## Scene Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3603d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scene Loader\n",
    "\n",
    "SPLIT = \"mini\"  # [\"mini\", \"test\", \"trainval\"]\n",
    "FILTER = \"all_scenes\"\n",
    "\n",
    "# hydra.initialize(config_path=\"./navsim/planning/script/config/common/train_test_split/scene_filter\")\n",
    "# cfg = hydra.compose(config_name=FILTER)\n",
    "\n",
    "scene_filter = SceneFilter(\n",
    "    num_history_frames=4,\n",
    "    num_future_frames=10,\n",
    "\n",
    ")\n",
    "\n",
    "cache_path = Path(os.getenv(\"CACHEDATA\"))\n",
    "openscene_data_root = Path(os.getenv(\"OPENSCENE_DATA_ROOT\"))\n",
    "\n",
    "scene_loader = SceneLoader(\n",
    "    openscene_data_root / f\"mini_navsim_logs/{SPLIT}\", # data_path\n",
    "    openscene_data_root / f\"mini_sensor_blobs/{SPLIT}\", # original_sensor_path\n",
    "    scene_filter,\n",
    "    openscene_data_root / \"warmup_two_stage/sensor_blobs\", # synthetic_sensor_path\n",
    "    openscene_data_root / \"warmup_two_stage/synthetic_scene_pickles\", # synthetic_scenes_path\n",
    "    sensor_config=SensorConfig.build_all_sensors(),\n",
    ")\n",
    "\n",
    "\n",
    "# Scene Data\n",
    "\n",
    "print(\"NUPLAN_MAPS_ROOT:\", os.environ[\"NUPLAN_MAPS_ROOT\"])\n",
    "print(\"Path exists:\", os.path.isdir(os.environ[\"NUPLAN_MAPS_ROOT\"]))\n",
    "\n",
    "token = np.random.choice(scene_loader.tokens)\n",
    "scene = scene_loader.get_scene_from_token(token)\n",
    "agent_input = scene.get_agent_input()\n",
    "    \n",
    "metadata = scene.scene_metadata\n",
    "token_path = cache_path / metadata.log_name / metadata.initial_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b7be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# from torchvision import transforms\n",
    "\n",
    "# cameras = agent_input.cameras[-1]\n",
    "# l0 = cameras.cam_l0.image[28:-28, 416:-416]\n",
    "# f0 = cameras.cam_f0.image[28:-28]\n",
    "# r0 = cameras.cam_r0.image[28:-28, 416:-416]\n",
    "# stitched_image = np.concatenate([l0, f0, r0], axis=1)\n",
    "# resized_image = cv2.resize(stitched_image, (1024, 256))\n",
    "# # resized_image = cv2.resize(stitched_image, (2048, 512))\n",
    "# tensor_image = transforms.ToTensor()(resized_image)\n",
    "\n",
    "# print(tensor_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ad035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Let's assume `agent_input` is your object\n",
    "for key, value in vars(agent_input).items():\n",
    "    if isinstance(value, list):\n",
    "        print(f\"{key}: list length = {len(value)}\")\n",
    "        for i, item in enumerate(value):\n",
    "            print(f\"  {i}: type = {type(item).__name__}\")\n",
    "            if hasattr(item, '__dict__'):  # For objects with attributes\n",
    "                for attr, attr_value in vars(item).items():\n",
    "                    if isinstance(attr_value, np.ndarray):\n",
    "                        print(f\"    {attr}: shape = {attr_value.shape}\")\n",
    "                    elif hasattr(attr_value, 'shape'):\n",
    "                        print(f\"    {attr}: shape = {attr_value.shape}\")\n",
    "            else:\n",
    "                print(f\"    No __dict__ for {type(item).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e22c044",
   "metadata": {},
   "source": [
    "## Feature Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df1644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from navsim.agents.diffusiondrive.transfuser_features import (\n",
    "    TransfuserFeatureBuilder, \n",
    "    TransfuserTargetBuilder\n",
    ")\n",
    "\n",
    "feature_builders = TransfuserFeatureBuilder(config=config)\n",
    "target_builders = TransfuserTargetBuilder(config=config)\n",
    "\n",
    "# Feature Builder\n",
    "batched_camera = []\n",
    "batched_lidar = []\n",
    "batched_status_feature = []\n",
    "\n",
    "for input in agent_input.cameras:  # list of AgentInput or similar\n",
    "    features = feature_builders._get_camera_feature(agent_input)\n",
    "    batched_camera.append(features)\n",
    "\n",
    "for input in agent_input.lidars:  # list of AgentInput or similar\n",
    "    features = feature_builders._get_lidar_feature(agent_input)\n",
    "    batched_lidar.append(features)\n",
    "\n",
    "for input in agent_input.ego_statuses:  # list of AgentInput or similar\n",
    "    features=torch.concatenate(\n",
    "            [\n",
    "                torch.tensor(agent_input.ego_statuses[-1].driving_command, dtype=torch.float32),\n",
    "                torch.tensor(agent_input.ego_statuses[-1].ego_velocity, dtype=torch.float32),\n",
    "                torch.tensor(agent_input.ego_statuses[-1].ego_acceleration, dtype=torch.float32),\n",
    "            ],\n",
    "        )    \n",
    "    batched_status_feature.append(features)\n",
    "\n",
    "\n",
    "camera_feature_batch = torch.stack(batched_camera, dim=0)  # (B, 3, 256, 1024)\n",
    "lidar_feature_batch = torch.stack(batched_lidar, dim=0)  # (B, 3, 256, 1024)\n",
    "status_feature_batch = torch.stack(batched_status_feature, dim=0)  # (B, 3, 256, 1024)\n",
    "batch_size = status_feature_batch.shape[0]\n",
    "\n",
    "print(camera_feature_batch.shape)\n",
    "print(lidar_feature_batch.shape)\n",
    "print(status_feature_batch.shape)\n",
    "\n",
    "# Target Builder (Future Trajectory)\n",
    "trajectory = torch.tensor(\n",
    "    scene.get_future_trajectory(num_trajectory_frames=config.trajectory_sampling.num_poses).poses\n",
    ")\n",
    "frame_idx = scene.scene_metadata.num_history_frames - 1\n",
    "annotations = scene.frames[frame_idx].annotations\n",
    "ego_pose = StateSE2(*scene.frames[frame_idx].ego_status.ego_pose)\n",
    "\n",
    "agent_states, agent_labels = target_builders._compute_agent_targets(annotations)\n",
    "bev_semantic_map = target_builders._compute_bev_semantic_map(annotations, scene.map_api, ego_pose)\n",
    "\n",
    "### Added the batch size for code running only\n",
    "trajectory = trajectory.unsqueeze(dim=0)\n",
    "trajectory = trajectory.repeat(4, 1, 1)  # [1, 8, 3]  [4, 8, 3]\n",
    "\n",
    "agent_states = agent_states.unsqueeze(dim=0)\n",
    "agent_states = agent_states.repeat(4, 1, 1)  # [1, 8, 3]  [4, 8, 3]\n",
    "\n",
    "agent_labels = agent_labels.unsqueeze(dim=0)\n",
    "agent_labels = agent_labels.repeat(4, 1)  # [1, 8, 3]  [4, 8, 3]\n",
    "\n",
    "bev_semantic_map = bev_semantic_map.unsqueeze(dim=0)\n",
    "bev_semantic_map = bev_semantic_map.repeat(4, 1, 1)  # [1, 8, 3]  [4, 8,\n",
    "\n",
    "targets = {\n",
    "    \"trajectory\": trajectory,\n",
    "    \"agent_states\": agent_states,\n",
    "    \"agent_labels\": agent_labels,\n",
    "    \"bev_semantic_map\": bev_semantic_map,\n",
    "}\n",
    "\n",
    "for key, value in targets.items():\n",
    "    print(f\"{key} = {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Function\n",
    "\n",
    "# BEV PROCESSING\n",
    "bev_feature_upscale, bev_feature, _ = backbone(camera_feature_batch, lidar_feature_batch)\n",
    "cross_bev_feature = bev_feature_upscale # [4, 64, 64, 64]\n",
    "bev_spatial_shape = bev_feature_upscale.shape[2:] # [64, 64]\n",
    "concat_cross_bev_shape = bev_feature.shape[2:] # [8, 8]\n",
    "bev_feature = bev_downscale(bev_feature) # [4, 512, 8, 8] -> [4, 256, 8, 8]\n",
    "bev_feature = bev_feature.flatten(-2, -1) # [4, 256, 64]\n",
    "bev_feature = bev_feature.permute(0, 2, 1) # [4, 64, 256]\n",
    "\n",
    "# ENCODING\n",
    "\n",
    "status_encoding_features = status_encoding(status_feature_batch) # [4, 256]\n",
    "\n",
    "keyval = torch.concatenate([bev_feature, status_encoding_features[:, None]], dim=1) # [4, 65, 256]\n",
    "keyval += keyval_embedding.weight[None, ...]\n",
    "\n",
    "concat_cross_bev = keyval[:,:-1].permute(0,2,1).contiguous().view(batch_size, -1, concat_cross_bev_shape[0], concat_cross_bev_shape[1]) # [4, 256, 64, 64]\n",
    "concat_cross_bev = F.interpolate(concat_cross_bev, size=bev_spatial_shape, mode='bilinear', align_corners=False)\n",
    "cross_bev_feature = torch.cat([concat_cross_bev, cross_bev_feature], dim=1) # [4, 320, 64, 64]\n",
    "cross_bev_feature = bev_proj(cross_bev_feature.flatten(-2,-1).permute(0,2,1)) # [4, 4096, 256]\n",
    "cross_bev_feature = cross_bev_feature.permute(0,2,1).contiguous().view(batch_size, -1, bev_spatial_shape[0], bev_spatial_shape[1]) # [4, 256, 64, 64]\n",
    "\n",
    "\n",
    "query = query_embedding.weight[None, ...].repeat(batch_size, 1, 1) # [4, 31, 256]\n",
    "query_out = tf_decoder(query, keyval) # [4, 31, 256]\n",
    "\n",
    "bev_semantic_map = bev_semantic_head(bev_feature_upscale) # [4, 7, 128, 256]\n",
    "\n",
    "# trajectory_query -> [4, 1, 256] / agent_query -> [4, 30, 256]\n",
    "trajectory_query, agents_query = query_out.split(query_splits, dim=1)\n",
    "\n",
    "output: Dict[str, torch.Tensor] = {\"bev_semantic_map\": bev_semantic_map}\n",
    "\n",
    "\n",
    "\n",
    "trajectory = trajectory_head(\n",
    "    trajectory_query,\n",
    "    agents_query, \n",
    "    cross_bev_feature,\n",
    "    bev_spatial_shape,\n",
    "    status_encoding_features[:, None],\n",
    "    targets=targets,\n",
    "    global_img=None\n",
    ")\n",
    "\n",
    "\n",
    "def print_dict_shapes(d, prefix=\"\"):\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            if value.dim() == 0:\n",
    "                print(f\"{prefix}{key}: scalar = {value.item():.4f}\")\n",
    "            else:\n",
    "                print(f\"{prefix}{key}: shape = {tuple(value.shape)}\")\n",
    "        elif isinstance(value, dict):\n",
    "            print(f\"{prefix}{key}:\")\n",
    "            print_dict_shapes(value, prefix=prefix + \"  \")\n",
    "        else:\n",
    "            print(f\"{prefix}{key}: {value} (type={type(value)})\")\n",
    "\n",
    "# Call it on your output\n",
    "print_dict_shapes(trajectory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d95fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trajectory_head.plan_anchor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0b5334",
   "metadata": {},
   "source": [
    "# Modification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51cd7ce",
   "metadata": {},
   "source": [
    "## Diffusion Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb0d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from OpenAI's diffusion repos\n",
    "#     GLIDE: https://github.com/openai/glide-text2im/blob/main/glide_text2im/gaussian_diffusion.py\n",
    "#     ADM:   https://github.com/openai/guided-diffusion/blob/main/guided_diffusion\n",
    "#     IDDPM: https://github.com/openai/improved-diffusion/blob/main/improved_diffusion/gaussian_diffusion.py\n",
    "\n",
    "import torch as th\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def normal_kl(mean1, logvar1, mean2, logvar2):\n",
    "    \"\"\"\n",
    "    Compute the KL divergence between two gaussians.\n",
    "    Shapes are automatically broadcasted, so batches can be compared to\n",
    "    scalars, among other use cases.\n",
    "    \"\"\"\n",
    "    tensor = None\n",
    "    for obj in (mean1, logvar1, mean2, logvar2):\n",
    "        if isinstance(obj, th.Tensor):\n",
    "            tensor = obj\n",
    "            break\n",
    "    assert tensor is not None, \"at least one argument must be a Tensor\"\n",
    "\n",
    "    # Force variances to be Tensors. Broadcasting helps convert scalars to\n",
    "    # Tensors, but it does not work for th.exp().\n",
    "    logvar1, logvar2 = [\n",
    "        x if isinstance(x, th.Tensor) else th.tensor(x).to(tensor)\n",
    "        for x in (logvar1, logvar2)\n",
    "    ]\n",
    "\n",
    "    return 0.5 * (\n",
    "        -1.0\n",
    "        + logvar2\n",
    "        - logvar1\n",
    "        + th.exp(logvar1 - logvar2)\n",
    "        + ((mean1 - mean2) ** 2) * th.exp(-logvar2)\n",
    "    )\n",
    "\n",
    "\n",
    "def approx_standard_normal_cdf(x):\n",
    "    \"\"\"\n",
    "    A fast approximation of the cumulative distribution function of the\n",
    "    standard normal.\n",
    "    \"\"\"\n",
    "    return 0.5 * (1.0 + th.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * th.pow(x, 3))))\n",
    "\n",
    "\n",
    "def continuous_gaussian_log_likelihood(x, *, means, log_scales):\n",
    "    \"\"\"\n",
    "    Compute the log-likelihood of a continuous Gaussian distribution.\n",
    "    :param x: the targets\n",
    "    :param means: the Gaussian mean Tensor.\n",
    "    :param log_scales: the Gaussian log stddev Tensor.\n",
    "    :return: a tensor like x of log probabilities (in nats).\n",
    "    \"\"\"\n",
    "    centered_x = x - means\n",
    "    inv_stdv = th.exp(-log_scales)\n",
    "    normalized_x = centered_x * inv_stdv\n",
    "    log_probs = th.distributions.Normal(th.zeros_like(x), th.ones_like(x)).log_prob(normalized_x)\n",
    "    return log_probs\n",
    "\n",
    "\n",
    "def discretized_gaussian_log_likelihood(x, *, means, log_scales):\n",
    "    \"\"\"\n",
    "    Compute the log-likelihood of a Gaussian distribution discretizing to a\n",
    "    given image.\n",
    "    :param x: the target images. It is assumed that this was uint8 values,\n",
    "              rescaled to the range [-1, 1].\n",
    "    :param means: the Gaussian mean Tensor.\n",
    "    :param log_scales: the Gaussian log stddev Tensor.\n",
    "    :return: a tensor like x of log probabilities (in nats).\n",
    "    \"\"\"\n",
    "    assert x.shape == means.shape == log_scales.shape\n",
    "    centered_x = x - means\n",
    "    inv_stdv = th.exp(-log_scales)\n",
    "    plus_in = inv_stdv * (centered_x + 1.0 / 255.0)\n",
    "    cdf_plus = approx_standard_normal_cdf(plus_in)\n",
    "    min_in = inv_stdv * (centered_x - 1.0 / 255.0)\n",
    "    cdf_min = approx_standard_normal_cdf(min_in)\n",
    "    log_cdf_plus = th.log(cdf_plus.clamp(min=1e-12))\n",
    "    log_one_minus_cdf_min = th.log((1.0 - cdf_min).clamp(min=1e-12))\n",
    "    cdf_delta = cdf_plus - cdf_min\n",
    "    log_probs = th.where(\n",
    "        x < -0.999,\n",
    "        log_cdf_plus,\n",
    "        th.where(x > 0.999, log_one_minus_cdf_min, th.log(cdf_delta.clamp(min=1e-12))),\n",
    "    )\n",
    "    assert log_probs.shape == x.shape\n",
    "    return log_probs    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a67cdb",
   "metadata": {},
   "source": [
    "## Dynamic Gaussian Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bcdcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[0.95]\n",
      "[1.]\n",
      "[0.]\n",
      "[0.22941573]\n"
     ]
    }
   ],
   "source": [
    "betas = [0.05]\n",
    "\n",
    "betas = np.array(betas, dtype=np.float64)\n",
    "betas = betas\n",
    "assert len(betas.shape) == 1, \"betas must be 1-D\"\n",
    "assert (betas > 0).all() and (betas <= 1).all()\n",
    "\n",
    "num_timesteps = int(betas.shape[0])\n",
    "\n",
    "alphas = 1.0 - betas\n",
    "print(alphas)\n",
    "alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n",
    "alphas_cumprod_next = np.append(alphas_cumprod[1:], 0.0)\n",
    "\n",
    "assert alphas_cumprod_prev.shape == (num_timesteps,)\n",
    "\n",
    "\n",
    "# calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "sqrt_alphas_cumprod = np.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - alphas_cumprod)\n",
    "log_one_minus_alphas_cumprod = np.log(1.0 - alphas_cumprod)\n",
    "sqrt_recip_alphas_cumprod = np.sqrt(1.0 / alphas_cumprod)\n",
    "sqrt_recipm1_alphas_cumprod = np.sqrt(1.0 / alphas_cumprod - 1)\n",
    "print(sqrt_recipm1_alphas_cumprod)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3fd2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import enum\n",
    "import torch\n",
    "\n",
    "def mean_flat(tensor):\n",
    "    \"\"\"\n",
    "    Take the mean over all non-batch dimensions.\n",
    "    \"\"\"\n",
    "    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n",
    "\n",
    "\n",
    "class ModelMeanType(enum.Enum):\n",
    "    \"\"\"\n",
    "    Which type of output the model predicts.\n",
    "    \"\"\"\n",
    "\n",
    "    PREVIOUS_X = enum.auto()  # the model predicts x_{t-1}\n",
    "    START_X = enum.auto()  # the model predicts x_0\n",
    "    EPSILON = enum.auto()  # the model predicts epsilon\n",
    "\n",
    "\n",
    "class ModelVarType(enum.Enum):\n",
    "    \"\"\"\n",
    "    What is used as the model's output variance.\n",
    "    The LEARNED_RANGE option has been added to allow the model to predict\n",
    "    values between FIXED_SMALL and FIXED_LARGE, making its job easier.\n",
    "    \"\"\"\n",
    "\n",
    "    LEARNED = enum.auto()\n",
    "    FIXED_SMALL = enum.auto()\n",
    "    FIXED_LARGE = enum.auto()\n",
    "    LEARNED_RANGE = enum.auto()\n",
    "\n",
    "\n",
    "class LossType(enum.Enum):\n",
    "    MSE = enum.auto()  # use raw MSE loss (and KL when learning variances)\n",
    "    RESCALED_MSE = (\n",
    "        enum.auto()\n",
    "    )  # use raw MSE loss (with RESCALED_KL when learning variances)\n",
    "    KL = enum.auto()  # use the variational lower-bound\n",
    "    RESCALED_KL = enum.auto()  # like KL, but rescale to estimate the full VLB\n",
    "\n",
    "    def is_vb(self):\n",
    "        return self == LossType.KL or self == LossType.RESCALED_KL\n",
    "\n",
    "\n",
    "def _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, warmup_frac):\n",
    "    betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n",
    "    warmup_time = int(num_diffusion_timesteps * warmup_frac)\n",
    "    betas[:warmup_time] = np.linspace(beta_start, beta_end, warmup_time, dtype=np.float64)\n",
    "    return betas\n",
    "\n",
    "\n",
    "def get_beta_schedule(beta_schedule, *, beta_start, beta_end, num_diffusion_timesteps):\n",
    "    \"\"\"\n",
    "    This is the deprecated API for creating beta schedules.\n",
    "    See get_named_beta_schedule() for the new library of schedules.\n",
    "    \"\"\"\n",
    "    if beta_schedule == \"quad\":\n",
    "        betas = (\n",
    "            np.linspace(\n",
    "                beta_start ** 0.5,\n",
    "                beta_end ** 0.5,\n",
    "                num_diffusion_timesteps,\n",
    "                dtype=np.float64,\n",
    "            )\n",
    "            ** 2\n",
    "        )\n",
    "    elif beta_schedule == \"linear\":\n",
    "        betas = np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float64)\n",
    "    elif beta_schedule == \"warmup10\":\n",
    "        betas = _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, 0.1)\n",
    "    elif beta_schedule == \"warmup50\":\n",
    "        betas = _warmup_beta(beta_start, beta_end, num_diffusion_timesteps, 0.5)\n",
    "    elif beta_schedule == \"const\":\n",
    "        betas = beta_end * np.ones(num_diffusion_timesteps, dtype=np.float64)\n",
    "    elif beta_schedule == \"jsd\":  # 1/T, 1/(T-1), 1/(T-2), ..., 1\n",
    "        betas = 1.0 / np.linspace(\n",
    "            num_diffusion_timesteps, 1, num_diffusion_timesteps, dtype=np.float64\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(beta_schedule)\n",
    "    assert betas.shape == (num_diffusion_timesteps,)\n",
    "    return betas\n",
    "\n",
    "\n",
    "def get_named_beta_schedule(schedule_name, num_diffusion_timesteps):\n",
    "    \"\"\"\n",
    "    Get a pre-defined beta schedule for the given name.\n",
    "    The beta schedule library consists of beta schedules which remain similar\n",
    "    in the limit of num_diffusion_timesteps.\n",
    "    Beta schedules may be added, but should not be removed or changed once\n",
    "    they are committed to maintain backwards compatibility.\n",
    "    \"\"\"\n",
    "    if schedule_name == \"linear\":\n",
    "        # Linear schedule from Ho et al, extended to work for any number of\n",
    "        # diffusion steps.\n",
    "        scale = 1000 / num_diffusion_timesteps\n",
    "        return get_beta_schedule(\n",
    "            \"linear\",\n",
    "            beta_start=scale * 0.0001,\n",
    "            beta_end=scale * 0.02,\n",
    "            num_diffusion_timesteps=num_diffusion_timesteps,\n",
    "        )\n",
    "    elif schedule_name == \"squaredcos_cap_v2\":\n",
    "        return betas_for_alpha_bar(\n",
    "            num_diffusion_timesteps,\n",
    "            lambda t: math.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2,\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f\"unknown beta schedule: {schedule_name}\")\n",
    "\n",
    "\n",
    "def betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n",
    "    \"\"\"\n",
    "    Create a beta schedule that discretizes the given alpha_t_bar function,\n",
    "    which defines the cumulative product of (1-beta) over time from t = [0,1].\n",
    "    :param num_diffusion_timesteps: the number of betas to produce.\n",
    "    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n",
    "                      produces the cumulative product of (1-beta) up to that\n",
    "                      part of the diffusion process.\n",
    "    :param max_beta: the maximum beta to use; use values lower than 1 to\n",
    "                     prevent singularities.\n",
    "    \"\"\"\n",
    "    betas = []\n",
    "    for i in range(num_diffusion_timesteps):\n",
    "        t1 = i / num_diffusion_timesteps\n",
    "        t2 = (i + 1) / num_diffusion_timesteps\n",
    "        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n",
    "    return np.array(betas)\n",
    "\n",
    "\n",
    "class GaussianDiffusion:\n",
    "    \"\"\"\n",
    "    Utilities for training and sampling diffusion models.\n",
    "    Original ported from this codebase:\n",
    "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py#L42\n",
    "    :param betas: a 1-D numpy array of betas for each diffusion timestep,\n",
    "                  starting at T and going to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        betas,\n",
    "        model_mean_type,\n",
    "        model_var_type,\n",
    "        loss_type\n",
    "    ):\n",
    "\n",
    "        self.model_mean_type = model_mean_type\n",
    "        self.model_var_type = model_var_type\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "        # Use float64 for accuracy.\n",
    "        betas = np.array(betas, dtype=np.float64)\n",
    "        self.betas = betas\n",
    "        assert len(betas.shape) == 1, \"betas must be 1-D\"\n",
    "        assert (betas > 0).all() and (betas <= 1).all()\n",
    "\n",
    "        self.num_timesteps = int(betas.shape[0])\n",
    "\n",
    "        alphas = 1.0 - betas\n",
    "        self.alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        self.alphas_cumprod_prev = np.append(1.0, self.alphas_cumprod[:-1])\n",
    "        self.alphas_cumprod_next = np.append(self.alphas_cumprod[1:], 0.0)\n",
    "        assert self.alphas_cumprod_prev.shape == (self.num_timesteps,)\n",
    "\n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)\n",
    "        self.log_one_minus_alphas_cumprod = np.log(1.0 - self.alphas_cumprod)\n",
    "        self.sqrt_recip_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod)\n",
    "        self.sqrt_recipm1_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod - 1)\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = (\n",
    "            betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
    "        self.posterior_log_variance_clipped = np.log(\n",
    "            np.append(self.posterior_variance[1], self.posterior_variance[1:])\n",
    "        ) if len(self.posterior_variance) > 1 else np.array([])\n",
    "\n",
    "        self.posterior_mean_coef1 = (\n",
    "            betas * np.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_mean_coef2 = (\n",
    "            (1.0 - self.alphas_cumprod_prev) * np.sqrt(alphas) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "\n",
    "    def q_mean_variance(self, x_start, t):\n",
    "        \"\"\"\n",
    "        Get the distribution q(x_t | x_0).\n",
    "        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n",
    "        \"\"\"\n",
    "        mean = _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "        variance = _extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n",
    "        log_variance = _extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "        return mean, variance, log_variance\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        \"\"\"\n",
    "        Diffuse the data for a given number of diffusion steps.\n",
    "        In other words, sample from q(x_t | x_0).\n",
    "        :param x_start: the initial data batch.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :param noise: if specified, the split-out normal noise.\n",
    "        :return: A noisy version of x_start.\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = th.randn_like(x_start)\n",
    "        assert noise.shape == x_start.shape\n",
    "        return (\n",
    "            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n",
    "            + _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n",
    "        )\n",
    "\n",
    "    def q_posterior_mean_variance(self, x_start, x_t, t):\n",
    "        \"\"\"\n",
    "        Compute the mean and variance of the diffusion posterior:\n",
    "            q(x_{t-1} | x_t, x_0)\n",
    "        \"\"\"\n",
    "        assert x_start.shape == x_t.shape\n",
    "        posterior_mean = (\n",
    "            _extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start\n",
    "            + _extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
    "        )\n",
    "        posterior_variance = _extract_into_tensor(self.posterior_variance, t, x_t.shape)\n",
    "        posterior_log_variance_clipped = _extract_into_tensor(\n",
    "            self.posterior_log_variance_clipped, t, x_t.shape\n",
    "        )\n",
    "        assert (\n",
    "            posterior_mean.shape[0]\n",
    "            == posterior_variance.shape[0]\n",
    "            == posterior_log_variance_clipped.shape[0]\n",
    "            == x_start.shape[0]\n",
    "        )\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def p_mean_variance(self, model, x, t, clip_denoised=True, denoised_fn=None, model_kwargs=None):\n",
    "        \"\"\"\n",
    "        Apply the model to get p(x_{t-1} | x_t), as well as a prediction of\n",
    "        the initial x, x_0.\n",
    "        :param model: the model, which takes a signal and a batch of timesteps\n",
    "                      as input.\n",
    "        :param x: the [N x C x ...] tensor at time t.\n",
    "        :param t: a 1-D Tensor of timesteps.\n",
    "        :param clip_denoised: if True, clip the denoised signal into [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample. Applies before\n",
    "            clip_denoised.\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :return: a dict with the following keys:\n",
    "                 - 'mean': the model mean output.\n",
    "                 - 'variance': the model variance output.\n",
    "                 - 'log_variance': the log of 'variance'.\n",
    "                 - 'pred_xstart': the prediction for x_0.\n",
    "        \"\"\"\n",
    "        if model_kwargs is None:\n",
    "            model_kwargs = {}\n",
    "\n",
    "        B, C = x.shape[:2]\n",
    "        assert t.shape == (B,)\n",
    "        model_output = model(x, t, **model_kwargs)\n",
    "        if isinstance(model_output, tuple):\n",
    "            model_output, extra = model_output\n",
    "        else:\n",
    "            extra = None\n",
    "\n",
    "        if self.model_var_type in [ModelVarType.LEARNED, ModelVarType.LEARNED_RANGE]:\n",
    "            assert model_output.shape == (B, C * 2, *x.shape[2:])\n",
    "            model_output, model_var_values = th.split(model_output, C, dim=1)\n",
    "            min_log = _extract_into_tensor(self.posterior_log_variance_clipped, t, x.shape)\n",
    "            max_log = _extract_into_tensor(np.log(self.betas), t, x.shape)\n",
    "            # The model_var_values is [-1, 1] for [min_var, max_var].\n",
    "            frac = (model_var_values + 1) / 2\n",
    "            model_log_variance = frac * max_log + (1 - frac) * min_log\n",
    "            model_variance = th.exp(model_log_variance)\n",
    "        else:\n",
    "            model_variance, model_log_variance = {\n",
    "                # for fixedlarge, we set the initial (log-)variance like so\n",
    "                # to get a better decoder log likelihood.\n",
    "                ModelVarType.FIXED_LARGE: (\n",
    "                    np.append(self.posterior_variance[1], self.betas[1:]),\n",
    "                    np.log(np.append(self.posterior_variance[1], self.betas[1:])),\n",
    "                ),\n",
    "                ModelVarType.FIXED_SMALL: (\n",
    "                    self.posterior_variance,\n",
    "                    self.posterior_log_variance_clipped,\n",
    "                ),\n",
    "            }[self.model_var_type]\n",
    "            model_variance = _extract_into_tensor(model_variance, t, x.shape)\n",
    "            model_log_variance = _extract_into_tensor(model_log_variance, t, x.shape)\n",
    "\n",
    "        def process_xstart(x):\n",
    "            if denoised_fn is not None:\n",
    "                x = denoised_fn(x)\n",
    "            if clip_denoised:\n",
    "                return x.clamp(-1, 1)\n",
    "            return x\n",
    "\n",
    "        if self.model_mean_type == ModelMeanType.START_X:\n",
    "            pred_xstart = process_xstart(model_output)\n",
    "        else:\n",
    "            pred_xstart = process_xstart(\n",
    "                self._predict_xstart_from_eps(x_t=x, t=t, eps=model_output)\n",
    "            )\n",
    "        model_mean, _, _ = self.q_posterior_mean_variance(x_start=pred_xstart, x_t=x, t=t)\n",
    "\n",
    "        assert model_mean.shape == model_log_variance.shape == pred_xstart.shape == x.shape\n",
    "        return {\n",
    "            \"mean\": model_mean,\n",
    "            \"variance\": model_variance,\n",
    "            \"log_variance\": model_log_variance,\n",
    "            \"pred_xstart\": pred_xstart,\n",
    "            \"extra\": extra,\n",
    "        }\n",
    "\n",
    "    def _predict_xstart_from_eps(self, x_t, t, eps):\n",
    "        assert x_t.shape == eps.shape\n",
    "        return (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n",
    "            - _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * eps\n",
    "        )\n",
    "\n",
    "    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n",
    "        return (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart\n",
    "        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n",
    "\n",
    "    def condition_mean(self, cond_fn, p_mean_var, x, t, model_kwargs=None):\n",
    "        \"\"\"\n",
    "        Compute the mean for the previous step, given a function cond_fn that\n",
    "        computes the gradient of a conditional log probability with respect to\n",
    "        x. In particular, cond_fn computes grad(log(p(y|x))), and we want to\n",
    "        condition on y.\n",
    "        This uses the conditioning strategy from Sohl-Dickstein et al. (2015).\n",
    "        \"\"\"\n",
    "        gradient = cond_fn(x, t, **model_kwargs)\n",
    "        new_mean = p_mean_var[\"mean\"].float() + p_mean_var[\"variance\"] * gradient.float()\n",
    "        return new_mean\n",
    "\n",
    "    def condition_score(self, cond_fn, p_mean_var, x, t, model_kwargs=None):\n",
    "        \"\"\"\n",
    "        Compute what the p_mean_variance output would have been, should the\n",
    "        model's score function be conditioned by cond_fn.\n",
    "        See condition_mean() for details on cond_fn.\n",
    "        Unlike condition_mean(), this instead uses the conditioning strategy\n",
    "        from Song et al (2020).\n",
    "        \"\"\"\n",
    "        alpha_bar = _extract_into_tensor(self.alphas_cumprod, t, x.shape)\n",
    "\n",
    "        eps = self._predict_eps_from_xstart(x, t, p_mean_var[\"pred_xstart\"])\n",
    "        eps = eps - (1 - alpha_bar).sqrt() * cond_fn(x, t, **model_kwargs)\n",
    "\n",
    "        out = p_mean_var.copy()\n",
    "        out[\"pred_xstart\"] = self._predict_xstart_from_eps(x, t, eps)\n",
    "        out[\"mean\"], _, _ = self.q_posterior_mean_variance(x_start=out[\"pred_xstart\"], x_t=x, t=t)\n",
    "        return out\n",
    "\n",
    "    def p_sample(\n",
    "        self,\n",
    "        model,\n",
    "        x,\n",
    "        t,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        cond_fn=None,\n",
    "        model_kwargs=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t-1} from the model at the given timestep.\n",
    "        :param model: the model to sample from.\n",
    "        :param x: the current tensor at x_{t-1}.\n",
    "        :param t: the value of t, starting at 0 for the first diffusion step.\n",
    "        :param clip_denoised: if True, clip the x_start prediction to [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample.\n",
    "        :param cond_fn: if not None, this is a gradient function that acts\n",
    "                        similarly to the model.\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :return: a dict containing the following keys:\n",
    "                 - 'sample': a random sample from the model.\n",
    "                 - 'pred_xstart': a prediction of x_0.\n",
    "        \"\"\"\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        noise = th.randn_like(x)\n",
    "        nonzero_mask = (\n",
    "            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n",
    "        )  # no noise when t == 0\n",
    "        if cond_fn is not None:\n",
    "            out[\"mean\"] = self.condition_mean(cond_fn, out, x, t, model_kwargs=model_kwargs)\n",
    "        sample = out[\"mean\"] + nonzero_mask * th.exp(0.5 * out[\"log_variance\"]) * noise\n",
    "        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n",
    "\n",
    "    def p_sample_loop(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        cond_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model.\n",
    "        :param model: the model module.\n",
    "        :param shape: the shape of the samples, (N, C, H, W).\n",
    "        :param noise: if specified, the noise from the encoder to sample.\n",
    "                      Should be of the same shape as `shape`.\n",
    "        :param clip_denoised: if True, clip x_start predictions to [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample.\n",
    "        :param cond_fn: if not None, this is a gradient function that acts\n",
    "                        similarly to the model.\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :param device: if specified, the device to create the samples on.\n",
    "                       If not specified, use a model parameter's device.\n",
    "        :param progress: if True, show a tqdm progress bar.\n",
    "        :return: a non-differentiable batch of samples.\n",
    "        \"\"\"\n",
    "        final = None\n",
    "        for sample in self.p_sample_loop_progressive(\n",
    "            model,\n",
    "            shape,\n",
    "            noise=noise,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            cond_fn=cond_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "            device=device,\n",
    "            progress=progress,\n",
    "        ):\n",
    "            final = sample\n",
    "        return final[\"sample\"]\n",
    "\n",
    "    def p_sample_loop_progressive(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        cond_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model and yield intermediate samples from\n",
    "        each timestep of diffusion.\n",
    "        Arguments are the same as p_sample_loop().\n",
    "        Returns a generator over dicts, where each dict is the return value of\n",
    "        p_sample().\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = next(model.parameters()).device\n",
    "        assert isinstance(shape, (tuple, list))\n",
    "        if noise is not None:\n",
    "            img = noise\n",
    "        else:\n",
    "            img = th.randn(*shape, device=device)\n",
    "        indices = list(range(self.num_timesteps))[::-1]\n",
    "\n",
    "        if progress:\n",
    "            # Lazy import so that we don't depend on tqdm.\n",
    "            from tqdm.auto import tqdm\n",
    "\n",
    "            indices = tqdm(indices)\n",
    "\n",
    "        for i in indices:\n",
    "            t = th.tensor([i] * shape[0], device=device)\n",
    "            with th.no_grad():\n",
    "                out = self.p_sample(\n",
    "                    model,\n",
    "                    img,\n",
    "                    t,\n",
    "                    clip_denoised=clip_denoised,\n",
    "                    denoised_fn=denoised_fn,\n",
    "                    cond_fn=cond_fn,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                )\n",
    "                yield out\n",
    "                img = out[\"sample\"]\n",
    "\n",
    "    def ddim_sample(\n",
    "        self,\n",
    "        model,\n",
    "        x,\n",
    "        t,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        cond_fn=None,\n",
    "        model_kwargs=None,\n",
    "        eta=0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t-1} from the model using DDIM.\n",
    "        Same usage as p_sample().\n",
    "        \"\"\"\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        if cond_fn is not None:\n",
    "            out = self.condition_score(cond_fn, out, x, t, model_kwargs=model_kwargs)\n",
    "\n",
    "        # Usually our model outputs epsilon, but we re-derive it\n",
    "        # in case we used x_start or x_prev prediction.\n",
    "        eps = self._predict_eps_from_xstart(x, t, out[\"pred_xstart\"])\n",
    "\n",
    "        alpha_bar = _extract_into_tensor(self.alphas_cumprod, t, x.shape)\n",
    "        alpha_bar_prev = _extract_into_tensor(self.alphas_cumprod_prev, t, x.shape)\n",
    "        sigma = (\n",
    "            eta\n",
    "            * th.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar))\n",
    "            * th.sqrt(1 - alpha_bar / alpha_bar_prev)\n",
    "        )\n",
    "        # Equation 12.\n",
    "        noise = th.randn_like(x)\n",
    "        mean_pred = (\n",
    "            out[\"pred_xstart\"] * th.sqrt(alpha_bar_prev)\n",
    "            + th.sqrt(1 - alpha_bar_prev - sigma ** 2) * eps\n",
    "        )\n",
    "        nonzero_mask = (\n",
    "            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n",
    "        )  # no noise when t == 0\n",
    "        sample = mean_pred + nonzero_mask * sigma * noise\n",
    "        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n",
    "\n",
    "    def ddim_reverse_sample(\n",
    "        self,\n",
    "        model,\n",
    "        x,\n",
    "        t,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        cond_fn=None,\n",
    "        model_kwargs=None,\n",
    "        eta=0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t+1} from the model using DDIM reverse ODE.\n",
    "        \"\"\"\n",
    "        assert eta == 0.0, \"Reverse ODE only for deterministic path\"\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        if cond_fn is not None:\n",
    "            out = self.condition_score(cond_fn, out, x, t, model_kwargs=model_kwargs)\n",
    "        # Usually our model outputs epsilon, but we re-derive it\n",
    "        # in case we used x_start or x_prev prediction.\n",
    "        eps = (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x.shape) * x\n",
    "            - out[\"pred_xstart\"]\n",
    "        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x.shape)\n",
    "        alpha_bar_next = _extract_into_tensor(self.alphas_cumprod_next, t, x.shape)\n",
    "\n",
    "        # Equation 12. reversed\n",
    "        mean_pred = out[\"pred_xstart\"] * th.sqrt(alpha_bar_next) + th.sqrt(1 - alpha_bar_next) * eps\n",
    "\n",
    "        return {\"sample\": mean_pred, \"pred_xstart\": out[\"pred_xstart\"]}\n",
    "\n",
    "    def ddim_sample_loop(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        cond_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        eta=0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model using DDIM.\n",
    "        Same usage as p_sample_loop().\n",
    "        \"\"\"\n",
    "        final = None\n",
    "        for sample in self.ddim_sample_loop_progressive(\n",
    "            model,\n",
    "            shape,\n",
    "            noise=noise,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            cond_fn=cond_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "            device=device,\n",
    "            progress=progress,\n",
    "            eta=eta,\n",
    "        ):\n",
    "            final = sample\n",
    "        return final[\"sample\"]\n",
    "\n",
    "    def ddim_sample_loop_progressive(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        cond_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        eta=0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Use DDIM to sample from the model and yield intermediate samples from\n",
    "        each timestep of DDIM.\n",
    "        Same usage as p_sample_loop_progressive().\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = next(model.parameters()).device\n",
    "        assert isinstance(shape, (tuple, list))\n",
    "        if noise is not None:\n",
    "            img = noise\n",
    "        else:\n",
    "            img = th.randn(*shape, device=device)\n",
    "        indices = list(range(self.num_timesteps))[::-1]\n",
    "\n",
    "        if progress:\n",
    "            # Lazy import so that we don't depend on tqdm.\n",
    "            from tqdm.auto import tqdm\n",
    "\n",
    "            indices = tqdm(indices)\n",
    "\n",
    "        for i in indices:\n",
    "            t = th.tensor([i] * shape[0], device=device)\n",
    "            with th.no_grad():\n",
    "                out = self.ddim_sample(\n",
    "                    model,\n",
    "                    img,\n",
    "                    t,\n",
    "                    clip_denoised=clip_denoised,\n",
    "                    denoised_fn=denoised_fn,\n",
    "                    cond_fn=cond_fn,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                    eta=eta,\n",
    "                )\n",
    "                yield out\n",
    "                img = out[\"sample\"]\n",
    "\n",
    "    def _vb_terms_bpd(\n",
    "            self, model, x_start, x_t, t, clip_denoised=True, model_kwargs=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Get a term for the variational lower-bound.\n",
    "        The resulting units are bits (rather than nats, as one might expect).\n",
    "        This allows for comparison to other papers.\n",
    "        :return: a dict with the following keys:\n",
    "                 - 'output': a shape [N] tensor of NLLs or KLs.\n",
    "                 - 'pred_xstart': the x_0 predictions.\n",
    "        \"\"\"\n",
    "        true_mean, _, true_log_variance_clipped = self.q_posterior_mean_variance(\n",
    "            x_start=x_start, x_t=x_t, t=t\n",
    "        )\n",
    "        out = self.p_mean_variance(\n",
    "            model, x_t, t, clip_denoised=clip_denoised, model_kwargs=model_kwargs\n",
    "        )\n",
    "        kl = normal_kl(\n",
    "            true_mean, true_log_variance_clipped, out[\"mean\"], out[\"log_variance\"]\n",
    "        )\n",
    "        kl = mean_flat(kl) / np.log(2.0)\n",
    "\n",
    "        decoder_nll = -discretized_gaussian_log_likelihood(\n",
    "            x_start, means=out[\"mean\"], log_scales=0.5 * out[\"log_variance\"]\n",
    "        )\n",
    "        assert decoder_nll.shape == x_start.shape\n",
    "        decoder_nll = mean_flat(decoder_nll) / np.log(2.0)\n",
    "\n",
    "        # At the first timestep return the decoder NLL,\n",
    "        # otherwise return KL(q(x_{t-1}|x_t,x_0) || p(x_{t-1}|x_t))\n",
    "        output = th.where((t == 0), decoder_nll, kl)\n",
    "        return {\"output\": output, \"pred_xstart\": out[\"pred_xstart\"]}\n",
    "\n",
    "\n",
    "    def _prior_bpd(self, x_start):\n",
    "        \"\"\"\n",
    "        Get the prior KL term for the variational lower-bound, measured in\n",
    "        bits-per-dim.\n",
    "        This term can't be optimized, as it only depends on the encoder.\n",
    "        :param x_start: the [N x C x ...] tensor of inputs.\n",
    "        :return: a batch of [N] KL values (in bits), one per batch element.\n",
    "        \"\"\"\n",
    "        batch_size = x_start.shape[0]\n",
    "        t = th.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n",
    "        qt_mean, _, qt_log_variance = self.q_mean_variance(x_start, t)\n",
    "        kl_prior = normal_kl(\n",
    "            mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0\n",
    "        )\n",
    "        return mean_flat(kl_prior) / np.log(2.0)\n",
    "\n",
    "    def calc_bpd_loop(self, model, x_start, clip_denoised=True, model_kwargs=None):\n",
    "        \"\"\"\n",
    "        Compute the entire variational lower-bound, measured in bits-per-dim,\n",
    "        as well as other related quantities.\n",
    "        :param model: the model to evaluate loss on.\n",
    "        :param x_start: the [N x C x ...] tensor of inputs.\n",
    "        :param clip_denoised: if True, clip denoised samples.\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :return: a dict containing the following keys:\n",
    "                 - total_bpd: the total variational lower-bound, per batch element.\n",
    "                 - prior_bpd: the prior term in the lower-bound.\n",
    "                 - vb: an [N x T] tensor of terms in the lower-bound.\n",
    "                 - xstart_mse: an [N x T] tensor of x_0 MSEs for each timestep.\n",
    "                 - mse: an [N x T] tensor of epsilon MSEs for each timestep.\n",
    "        \"\"\"\n",
    "        device = x_start.device\n",
    "        batch_size = x_start.shape[0]\n",
    "\n",
    "        vb = []\n",
    "        xstart_mse = []\n",
    "        mse = []\n",
    "        for t in list(range(self.num_timesteps))[::-1]:\n",
    "            t_batch = th.tensor([t] * batch_size, device=device)\n",
    "            noise = th.randn_like(x_start)\n",
    "            x_t = self.q_sample(x_start=x_start, t=t_batch, noise=noise)\n",
    "            # Calculate VLB term at the current timestep\n",
    "            with th.no_grad():\n",
    "                out = self._vb_terms_bpd(\n",
    "                    model,\n",
    "                    x_start=x_start,\n",
    "                    x_t=x_t,\n",
    "                    t=t_batch,\n",
    "                    clip_denoised=clip_denoised,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                )\n",
    "            vb.append(out[\"output\"])\n",
    "            xstart_mse.append(mean_flat((out[\"pred_xstart\"] - x_start) ** 2))\n",
    "            eps = self._predict_eps_from_xstart(x_t, t_batch, out[\"pred_xstart\"])\n",
    "            mse.append(mean_flat((eps - noise) ** 2))\n",
    "\n",
    "        vb = th.stack(vb, dim=1)\n",
    "        xstart_mse = th.stack(xstart_mse, dim=1)\n",
    "        mse = th.stack(mse, dim=1)\n",
    "\n",
    "        prior_bpd = self._prior_bpd(x_start)\n",
    "        total_bpd = vb.sum(dim=1) + prior_bpd\n",
    "        return {\n",
    "            \"total_bpd\": total_bpd,\n",
    "            \"prior_bpd\": prior_bpd,\n",
    "            \"vb\": vb,\n",
    "            \"xstart_mse\": xstart_mse,\n",
    "            \"mse\": mse,\n",
    "        }\n",
    "\n",
    "\n",
    "def _extract_into_tensor(arr, timesteps, broadcast_shape):\n",
    "    \"\"\"\n",
    "    Extract values from a 1-D numpy array for a batch of indices.\n",
    "    :param arr: the 1-D numpy array.\n",
    "    :param timesteps: a tensor of indices into the array to extract.\n",
    "    :param broadcast_shape: a larger shape of K dimensions with the batch\n",
    "                            dimension equal to the length of timesteps.\n",
    "    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n",
    "    \"\"\"\n",
    "    res = th.from_numpy(arr).to(device=timesteps.device)[timesteps].float()\n",
    "    while len(res.shape) < len(broadcast_shape):\n",
    "        res = res[..., None]\n",
    "    return res + th.zeros(broadcast_shape, device=timesteps.device)\n",
    "\n",
    "\n",
    "def space_timesteps(num_timesteps, section_counts):\n",
    "    \"\"\"\n",
    "    Create a list of timesteps to use from an original diffusion process,\n",
    "    given the number of timesteps we want to take from equally-sized portions\n",
    "    of the original process.\n",
    "    For example, if there's 300 timesteps and the section counts are [10,15,20]\n",
    "    then the first 100 timesteps are strided to be 10 timesteps, the second 100\n",
    "    are strided to be 15 timesteps, and the final 100 are strided to be 20.\n",
    "    If the stride is a string starting with \"ddim\", then the fixed striding\n",
    "    from the DDIM paper is used, and only one section is allowed.\n",
    "    :param num_timesteps: the number of diffusion steps in the original\n",
    "                          process to divide up.\n",
    "    :param section_counts: either a list of numbers, or a string containing\n",
    "                           comma-separated numbers, indicating the step count\n",
    "                           per section. As a special case, use \"ddimN\" where N\n",
    "                           is a number of steps to use the striding from the\n",
    "                           DDIM paper.\n",
    "    :return: a set of diffusion steps from the original process to use.\n",
    "    \"\"\"\n",
    "    if isinstance(section_counts, str):\n",
    "        if section_counts.startswith(\"ddim\"):\n",
    "            desired_count = int(section_counts[len(\"ddim\") :])\n",
    "            for i in range(1, num_timesteps):\n",
    "                if len(range(0, num_timesteps, i)) == desired_count:\n",
    "                    return set(range(0, num_timesteps, i))\n",
    "            raise ValueError(\n",
    "                f\"cannot create exactly {num_timesteps} steps with an integer stride\"\n",
    "            )\n",
    "        section_counts = [int(x) for x in section_counts.split(\",\")]\n",
    "    size_per = num_timesteps // len(section_counts)\n",
    "    extra = num_timesteps % len(section_counts)\n",
    "    start_idx = 0\n",
    "    all_steps = []\n",
    "    for i, section_count in enumerate(section_counts):\n",
    "        size = size_per + (1 if i < extra else 0)\n",
    "        if size < section_count:\n",
    "            raise ValueError(\n",
    "                f\"cannot divide section of {size} steps into {section_count}\"\n",
    "            )\n",
    "        if section_count <= 1:\n",
    "            frac_stride = 1\n",
    "        else:\n",
    "            frac_stride = (size - 1) / (section_count - 1)\n",
    "        cur_idx = 0.0\n",
    "        taken_steps = []\n",
    "        for _ in range(section_count):\n",
    "            taken_steps.append(start_idx + round(cur_idx))\n",
    "            cur_idx += frac_stride\n",
    "        all_steps += taken_steps\n",
    "        start_idx += size\n",
    "    return set(all_steps)\n",
    "\n",
    "\n",
    "class SpacedDiffusion(GaussianDiffusion):\n",
    "    \"\"\"\n",
    "    A diffusion process which can skip steps in a base diffusion process.\n",
    "    :param use_timesteps: a collection (sequence or set) of timesteps from the\n",
    "                          original diffusion process to retain.\n",
    "    :param kwargs: the kwargs to create the base diffusion process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_timesteps, **kwargs):\n",
    "        self.use_timesteps = set(use_timesteps)\n",
    "        self.timestep_map = []\n",
    "        self.original_num_steps = len(kwargs[\"betas\"])\n",
    "\n",
    "        base_diffusion = GaussianDiffusion(**kwargs)  # pylint: disable=missing-kwoa\n",
    "        last_alpha_cumprod = 1.0\n",
    "        new_betas = []\n",
    "        for i, alpha_cumprod in enumerate(base_diffusion.alphas_cumprod):\n",
    "            if i in self.use_timesteps:\n",
    "                new_betas.append(1 - alpha_cumprod / last_alpha_cumprod)\n",
    "                last_alpha_cumprod = alpha_cumprod\n",
    "                self.timestep_map.append(i)\n",
    "        kwargs[\"betas\"] = np.array(new_betas)\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def p_mean_variance(\n",
    "        self, model, *args, **kwargs\n",
    "    ):  # pylint: disable=signature-differs\n",
    "        return super().p_mean_variance(self._wrap_model(model), *args, **kwargs)\n",
    "\n",
    "    def training_losses(\n",
    "        self, model, *args, **kwargs\n",
    "    ):  # pylint: disable=signature-differs\n",
    "        return super().training_losses(self._wrap_model(model), *args, **kwargs)\n",
    "\n",
    "    def training_losses_without_completemodel(\n",
    "        self, model, *args, **kwargs\n",
    "    ):  # pylint: disable=signature-differs\n",
    "        return super().training_losses_without_completemodel(self._wrap_model(model), *args, **kwargs)\n",
    "\n",
    "    def condition_mean(self, cond_fn, *args, **kwargs):\n",
    "        return super().condition_mean(self._wrap_model(cond_fn), *args, **kwargs)\n",
    "\n",
    "    def condition_score(self, cond_fn, *args, **kwargs):\n",
    "        return super().condition_score(self._wrap_model(cond_fn), *args, **kwargs)\n",
    "\n",
    "    def _wrap_model(self, model):\n",
    "        if isinstance(model, _WrappedModel):\n",
    "            return model\n",
    "        return _WrappedModel(\n",
    "            model, self.timestep_map, self.original_num_steps\n",
    "        )\n",
    "\n",
    "    def _scale_timesteps(self, t):\n",
    "        # Scaling is done by the wrapped model.\n",
    "        return t\n",
    "\n",
    "\n",
    "class _WrappedModel:\n",
    "    def __init__(self, model, timestep_map, original_num_steps):\n",
    "        self.model = model\n",
    "        self.timestep_map = timestep_map\n",
    "        # self.rescale_timesteps = rescale_timesteps\n",
    "        self.original_num_steps = original_num_steps\n",
    "\n",
    "    def __call__(self, x, ts, **kwargs):\n",
    "        map_tensor = th.tensor(self.timestep_map, device=ts.device, dtype=ts.dtype)\n",
    "        new_ts = map_tensor[ts]\n",
    "        # if self.rescale_timesteps:\n",
    "        #     new_ts = new_ts.float() * (1000.0 / self.original_num_steps)\n",
    "        return self.model(x, new_ts, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6e35abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Trajectory Head\n",
    "\n",
    "# Module\n",
    "\n",
    "class DiffMotionPlanningRefinementModule(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dims=256,\n",
    "        ego_fut_ts=8,\n",
    "        ego_fut_mode=20,\n",
    "        if_zeroinit_reg=True,\n",
    "    ):\n",
    "        super(DiffMotionPlanningRefinementModule, self).__init__()\n",
    "        self.embed_dims = embed_dims\n",
    "        self.ego_fut_ts = ego_fut_ts\n",
    "        self.ego_fut_mode = ego_fut_mode\n",
    "        self.plan_cls_branch = nn.Sequential(\n",
    "            *linear_relu_ln(embed_dims, 1, 2),\n",
    "            nn.Linear(embed_dims, 1),\n",
    "        )\n",
    "        self.plan_reg_branch = nn.Sequential(\n",
    "            nn.Linear(embed_dims, embed_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dims, embed_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dims, ego_fut_ts * 3),\n",
    "        )\n",
    "        self.if_zeroinit_reg = False\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        if self.if_zeroinit_reg:\n",
    "            nn.init.constant_(self.plan_reg_branch[-1].weight, 0)\n",
    "            nn.init.constant_(self.plan_reg_branch[-1].bias, 0)\n",
    "\n",
    "        bias_init = bias_init_with_prob(0.01)\n",
    "        nn.init.constant_(self.plan_cls_branch[-1].bias, bias_init)\n",
    "    def forward(\n",
    "        self,\n",
    "        traj_feature,\n",
    "    ):\n",
    "        bs, ego_fut_mode, _ = traj_feature.shape\n",
    "\n",
    "        # 6. get final prediction\n",
    "        traj_feature = traj_feature.view(bs, ego_fut_mode,-1)\n",
    "        plan_cls = self.plan_cls_branch(traj_feature).squeeze(-1)\n",
    "        traj_delta = self.plan_reg_branch(traj_feature)\n",
    "        plan_reg = traj_delta.reshape(bs,ego_fut_mode, self.ego_fut_ts, 3)\n",
    "\n",
    "        return plan_reg, plan_cls\n",
    "    \n",
    "class ModulationLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dims: int, condition_dims: int):\n",
    "        super(ModulationLayer, self).__init__()\n",
    "        self.if_zeroinit_scale=False\n",
    "        self.embed_dims = embed_dims\n",
    "        self.scale_shift_mlp = nn.Sequential(\n",
    "            nn.Mish(),\n",
    "            nn.Linear(condition_dims, embed_dims*2),\n",
    "        )\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        if self.if_zeroinit_scale:\n",
    "            nn.init.constant_(self.scale_shift_mlp[-1].weight, 0)\n",
    "            nn.init.constant_(self.scale_shift_mlp[-1].bias, 0)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        traj_feature,\n",
    "        time_embed,\n",
    "        global_cond=None,\n",
    "        global_img=None,\n",
    "    ):\n",
    "        if global_cond is not None:\n",
    "            global_feature = torch.cat([\n",
    "                    global_cond, time_embed\n",
    "                ], axis=-1)\n",
    "        else:\n",
    "            global_feature = time_embed\n",
    "        if global_img is not None:\n",
    "            global_img = global_img.flatten(2,3).permute(0,2,1).contiguous()\n",
    "            global_feature = torch.cat([\n",
    "                    global_img, global_feature\n",
    "                ], axis=-1)\n",
    "        \n",
    "        print(global_feature)\n",
    "        scale_shift = self.scale_shift_mlp(global_feature)\n",
    "        scale,shift = scale_shift.chunk(2,dim=-1)\n",
    "        traj_feature = traj_feature * (1 + scale) + shift\n",
    "        return traj_feature\n",
    "\n",
    "# Decoder Layer\n",
    "\n",
    "class CustomTransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_poses,\n",
    "                 d_model,\n",
    "                 d_ffn,\n",
    "                 config,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.cross_bev_attention = GridSampleCrossBEVAttention(\n",
    "            config.tf_d_model,\n",
    "            config.tf_num_head,\n",
    "            num_points=num_poses,\n",
    "            config=config,\n",
    "            in_bev_dims=256,\n",
    "        )\n",
    "        self.cross_agent_attention = nn.MultiheadAttention(\n",
    "            config.tf_d_model,\n",
    "            config.tf_num_head,\n",
    "            dropout=config.tf_dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.cross_ego_attention = nn.MultiheadAttention(\n",
    "            config.tf_d_model,\n",
    "            config.tf_num_head,\n",
    "            dropout=config.tf_dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config.tf_d_model, config.tf_d_ffn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config.tf_d_ffn, config.tf_d_model),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(config.tf_d_model)\n",
    "        self.norm2 = nn.LayerNorm(config.tf_d_model)\n",
    "        self.norm3 = nn.LayerNorm(config.tf_d_model)\n",
    "        self.time_modulation = ModulationLayer(config.tf_d_model,256)\n",
    "        self.task_decoder = DiffMotionPlanningRefinementModule(\n",
    "            embed_dims=config.tf_d_model,\n",
    "            ego_fut_ts=num_poses,\n",
    "            ego_fut_mode=20,\n",
    "        )\n",
    "\n",
    "    def forward(self, \n",
    "                traj_feature, \n",
    "                noisy_traj_points, \n",
    "                bev_feature, \n",
    "                bev_spatial_shape, \n",
    "                agents_query, \n",
    "                ego_query, \n",
    "                time_embed, \n",
    "                status_encoding,\n",
    "                global_img=None):\n",
    "        traj_feature = self.cross_bev_attention(traj_feature,noisy_traj_points,bev_feature,bev_spatial_shape)\n",
    "        traj_feature = traj_feature + self.dropout(self.cross_agent_attention(traj_feature, agents_query,agents_query)[0])\n",
    "        traj_feature = self.norm1(traj_feature)\n",
    "        \n",
    "        # traj_feature = traj_feature + self.dropout(self.self_attn(traj_feature, traj_feature, traj_feature)[0])\n",
    "\n",
    "        # 4.5 cross attention with  ego query\n",
    "        traj_feature = traj_feature + self.dropout1(self.cross_ego_attention(traj_feature, ego_query,ego_query)[0])\n",
    "        traj_feature = self.norm2(traj_feature)\n",
    "        \n",
    "        # 4.6 feedforward network\n",
    "        traj_feature = self.norm3(self.ffn(traj_feature))\n",
    "        # 4.8 modulate with time steps\n",
    "        traj_feature = self.time_modulation(traj_feature, time_embed,global_cond=None,global_img=global_img)\n",
    "        \n",
    "        # 4.9 predict the offset & heading\n",
    "        poses_reg, poses_cls = self.task_decoder(traj_feature) #bs,20,8,3; bs,20\n",
    "        poses_reg[...,:2] = poses_reg[...,:2] + noisy_traj_points\n",
    "        poses_reg[..., StateSE2Index.HEADING] = poses_reg[..., StateSE2Index.HEADING].tanh() * np.pi\n",
    "\n",
    "        return poses_reg, poses_cls\n",
    "def _get_clones(module, N):\n",
    "    # FIXME: copy.deepcopy() is not defined on nn.module\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "# Decoder\n",
    "\n",
    "class CustomTransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        decoder_layer, \n",
    "        num_layers,\n",
    "        norm=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        torch._C._log_api_usage_once(f\"torch.nn.modules.{self.__class__.__name__}\")\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self, \n",
    "                traj_feature, \n",
    "                noisy_traj_points, \n",
    "                bev_feature, \n",
    "                bev_spatial_shape, \n",
    "                agents_query, \n",
    "                ego_query, \n",
    "                time_embed, \n",
    "                status_encoding,\n",
    "                global_img=None):\n",
    "        poses_reg_list = []\n",
    "        poses_cls_list = []\n",
    "        traj_points = noisy_traj_points\n",
    "        for mod in self.layers:\n",
    "            poses_reg, poses_cls = mod(traj_feature, traj_points, bev_feature, bev_spatial_shape, agents_query, ego_query, time_embed, status_encoding,global_img)\n",
    "            poses_reg_list.append(poses_reg)\n",
    "            poses_cls_list.append(poses_cls)\n",
    "            traj_points = poses_reg[...,:2].clone().detach()\n",
    "        return poses_reg_list, poses_cls_list\n",
    "\n",
    "\n",
    "class TrajectoryHead(nn.Module):\n",
    "    \"\"\"Trajectory prediction head.\"\"\"\n",
    "\n",
    "    def __init__(self, num_poses: int, d_ffn: int, d_model: int, plan_anchor_path: str,config: TransfuserConfig):\n",
    "        \"\"\"\n",
    "        Initializes trajectory head.\n",
    "        :param num_poses: number of (x,y,) poses to predict\n",
    "        :param d_ffn: dimensionality of feed-forward network\n",
    "        :param d_model: input dimensionality\n",
    "        \"\"\"\n",
    "        super(TrajectoryHead, self).__init__()\n",
    "\n",
    "        self._num_poses = num_poses\n",
    "        self._d_model = d_model\n",
    "        self._d_ffn = d_ffn\n",
    "        self.diff_loss_weight = 2.0\n",
    "        self.ego_fut_mode = 20\n",
    "\n",
    "        self.spaced_diffusion = SpacedDiffusion(\n",
    "            use_timesteps=1000,\n",
    "        )\n",
    "\n",
    "\n",
    "        plan_anchor = np.load(plan_anchor_path)\n",
    "\n",
    "        self.plan_anchor = nn.Parameter(\n",
    "            torch.tensor(plan_anchor, dtype=torch.float32),\n",
    "            requires_grad=False,\n",
    "        ) # 20,8,2\n",
    "        self.plan_anchor_encoder = nn.Sequential(\n",
    "            *linear_relu_ln(d_model, 1, 1,512),\n",
    "            nn.Linear(d_model, d_model),\n",
    "        )\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPosEmb(d_model),\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.Mish(),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "        )\n",
    "\n",
    "        diff_decoder_layer = CustomTransformerDecoderLayer(\n",
    "            num_poses=num_poses,\n",
    "            d_model=d_model,\n",
    "            d_ffn=d_ffn,\n",
    "            config=config,\n",
    "        )\n",
    "        self.diff_decoder = CustomTransformerDecoder(diff_decoder_layer, 2)\n",
    "\n",
    "        self.loss_computer = LossComputer(config)\n",
    "    def norm_odo(self, odo_info_fut):\n",
    "        odo_info_fut_x = odo_info_fut[..., 0:1]\n",
    "        odo_info_fut_y = odo_info_fut[..., 1:2]\n",
    "        odo_info_fut_head = odo_info_fut[..., 2:3]\n",
    "\n",
    "        odo_info_fut_x = 2*(odo_info_fut_x + 1.2)/56.9 -1\n",
    "        odo_info_fut_y = 2*(odo_info_fut_y + 20)/46 -1\n",
    "        odo_info_fut_head = 2*(odo_info_fut_head + 2)/3.9 -1\n",
    "        return torch.cat([odo_info_fut_x, odo_info_fut_y, odo_info_fut_head], dim=-1)\n",
    "    def denorm_odo(self, odo_info_fut):\n",
    "        odo_info_fut_x = odo_info_fut[..., 0:1]\n",
    "        odo_info_fut_y = odo_info_fut[..., 1:2]\n",
    "        odo_info_fut_head = odo_info_fut[..., 2:3]\n",
    "\n",
    "        odo_info_fut_x = (odo_info_fut_x + 1)/2 * 56.9 - 1.2\n",
    "        odo_info_fut_y = (odo_info_fut_y + 1)/2 * 46 - 20\n",
    "        odo_info_fut_head = (odo_info_fut_head + 1)/2 * 3.9 - 2\n",
    "        return torch.cat([odo_info_fut_x, odo_info_fut_y, odo_info_fut_head], dim=-1)\n",
    "    def forward(self, ego_query, agents_query, bev_feature,bev_spatial_shape,status_encoding, targets=None,global_img=None) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Torch module forward pass.\"\"\"\n",
    "        if self.training:\n",
    "            return self.forward_train(ego_query, agents_query, bev_feature,bev_spatial_shape,status_encoding,targets,global_img)\n",
    "        else:\n",
    "            return self.forward_test(ego_query, agents_query, bev_feature,bev_spatial_shape,status_encoding,global_img)\n",
    "\n",
    "\n",
    "    def forward_train(self, ego_query,agents_query,bev_feature,bev_spatial_shape,status_encoding, targets=None,global_img=None) -> Dict[str, torch.Tensor]:\n",
    "        bs = ego_query.shape[0]\n",
    "        device = ego_query.device\n",
    "        # 1. add truncated noise to the plan anchor\n",
    "        plan_anchor = self.plan_anchor.unsqueeze(0).repeat(bs,1,1,1)\n",
    "        odo_info_fut = self.norm_odo(plan_anchor)\n",
    "        timesteps = torch.randint(\n",
    "            0, 50,\n",
    "            (bs,), device=device\n",
    "        )\n",
    "\n",
    "\n",
    "        # Diffusion\n",
    "        noise = torch.randn(odo_info_fut.shape, device=device)\n",
    "        noisy_traj_points = self.diffusion_scheduler.add_noise(\n",
    "            original_samples=odo_info_fut,\n",
    "            noise=noise,\n",
    "            timesteps=timesteps,\n",
    "        ).float()\n",
    "        noisy_traj_points = torch.clamp(noisy_traj_points, min=-1, max=1)\n",
    "        noisy_traj_points = self.denorm_odo(noisy_traj_points)\n",
    "\n",
    "        ego_fut_mode = noisy_traj_points.shape[1]\n",
    "        # 2. proj noisy_traj_points to the query\n",
    "        traj_pos_embed = gen_sineembed_for_position(noisy_traj_points,hidden_dim=64)\n",
    "        traj_pos_embed = traj_pos_embed.flatten(-2)\n",
    "        traj_feature = self.plan_anchor_encoder(traj_pos_embed)\n",
    "        traj_feature = traj_feature.view(bs,ego_fut_mode,-1)\n",
    "        # 3. embed the timesteps\n",
    "        time_embed = self.time_mlp(timesteps)\n",
    "        time_embed = time_embed.view(bs,1,-1)\n",
    "\n",
    "\n",
    "        # 4. begin the stacked decoder\n",
    "        poses_reg_list, poses_cls_list = self.diff_decoder(traj_feature, noisy_traj_points, bev_feature, bev_spatial_shape, agents_query, ego_query, time_embed, status_encoding,global_img)\n",
    "\n",
    "        trajectory_loss_dict = {}\n",
    "        ret_traj_loss = 0\n",
    "        for idx, (poses_reg, poses_cls) in enumerate(zip(poses_reg_list, poses_cls_list)):\n",
    "            trajectory_loss = self.loss_computer(poses_reg, poses_cls, targets, plan_anchor)\n",
    "            trajectory_loss_dict[f\"trajectory_loss_{idx}\"] = trajectory_loss\n",
    "            ret_traj_loss += trajectory_loss\n",
    "\n",
    "        mode_idx = poses_cls_list[-1].argmax(dim=-1)\n",
    "        mode_idx = mode_idx[...,None,None,None].repeat(1,1,self._num_poses,3)\n",
    "        best_reg = torch.gather(poses_reg_list[-1], 1, mode_idx).squeeze(1)\n",
    "        return {\"trajectory\": best_reg,\"trajectory_loss\":ret_traj_loss,\"trajectory_loss_dict\":trajectory_loss_dict}\n",
    "\n",
    "    def forward_test(self, ego_query,agents_query,bev_feature,bev_spatial_shape,status_encoding,global_img) -> Dict[str, torch.Tensor]:\n",
    "        step_num = 2\n",
    "        bs = ego_query.shape[0]\n",
    "        device = ego_query.device\n",
    "        self.diffusion_scheduler.set_timesteps(1000, device)\n",
    "        step_ratio = 20 / step_num\n",
    "        roll_timesteps = (np.arange(0, step_num) * step_ratio).round()[::-1].copy().astype(np.int64)\n",
    "        roll_timesteps = torch.from_numpy(roll_timesteps).to(device)\n",
    "\n",
    "\n",
    "        # 1. add truncated noise to the plan anchor\n",
    "        plan_anchor = self.plan_anchor.unsqueeze(0).repeat(bs,1,1,1)\n",
    "        img = self.norm_odo(plan_anchor)\n",
    "        noise = torch.randn(img.shape, device=device)\n",
    "        trunc_timesteps = torch.ones((bs,), device=device, dtype=torch.long) * 8\n",
    "        img = self.diffusion_scheduler.add_noise(original_samples=img, noise=noise, timesteps=trunc_timesteps)\n",
    "        noisy_trajs = self.denorm_odo(img)\n",
    "        ego_fut_mode = img.shape[1]\n",
    "        for k in roll_timesteps[:]:\n",
    "            x_boxes = torch.clamp(img, min=-1, max=1)\n",
    "            noisy_traj_points = self.denorm_odo(x_boxes)\n",
    "\n",
    "            # 2. proj noisy_traj_points to the query\n",
    "            traj_pos_embed = gen_sineembed_for_position(noisy_traj_points,hidden_dim=64)\n",
    "            traj_pos_embed = traj_pos_embed.flatten(-2)\n",
    "            traj_feature = self.plan_anchor_encoder(traj_pos_embed)\n",
    "            traj_feature = traj_feature.view(bs,ego_fut_mode,-1)\n",
    "\n",
    "            timesteps = k\n",
    "            if not torch.is_tensor(timesteps):\n",
    "                # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "                timesteps = torch.tensor([timesteps], dtype=torch.long, device=img.device)\n",
    "            elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:\n",
    "                timesteps = timesteps[None].to(img.device)\n",
    "            \n",
    "            # 3. embed the timesteps\n",
    "            timesteps = timesteps.expand(img.shape[0])\n",
    "            time_embed = self.time_mlp(timesteps)\n",
    "            time_embed = time_embed.view(bs,1,-1)\n",
    "\n",
    "            # 4. begin the stacked decoder\n",
    "            poses_reg_list, poses_cls_list = self.diff_decoder(traj_feature, noisy_traj_points, bev_feature, bev_spatial_shape, agents_query, ego_query, time_embed, status_encoding,global_img)\n",
    "            poses_reg = poses_reg_list[-1]\n",
    "            poses_cls = poses_cls_list[-1]\n",
    "            x_start = poses_reg[...,:2]\n",
    "            x_start = self.norm_odo(x_start)\n",
    "            img = self.diffusion_scheduler.step(\n",
    "                model_output=x_start,\n",
    "                timestep=k,\n",
    "                sample=img\n",
    "            ).prev_sample\n",
    "        mode_idx = poses_cls.argmax(dim=-1)\n",
    "        mode_idx = mode_idx[...,None,None,None].repeat(1,1,self._num_poses,3)\n",
    "        best_reg = torch.gather(poses_reg, 1, mode_idx).squeeze(1)\n",
    "        return {\"trajectory\": best_reg}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1630f11b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "navsim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
